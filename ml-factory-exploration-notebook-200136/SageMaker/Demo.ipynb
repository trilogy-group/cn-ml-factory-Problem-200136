{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
       "// on document structure change\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
    "// on document structure change\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764\n"
     ]
    }
   ],
   "source": [
    "# Parameter: URL of the REST API\n",
    "%env MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "# Problem is created in UI, here we use its ID as parameter\n",
    "problem_id = 200136\n",
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Experiment is pre-created in UI, here we use its ID as parameter to load and reuse it\n",
    "experiment_id = 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3transfer-0.3.3.dist-info/METADATA'\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install MLFactory SDK\n",
    "!pip install mlfactory_sdk --upgrade --extra-index https://pypi.swarm.devfactory.com > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    argon2-cffi-20.1.0         |   py36h7b6447c_1          46 KB\n",
      "    astroid-2.4.2              |           py36_0         279 KB\n",
      "    async_generator-1.10       |   py36h28b3542_0          39 KB\n",
      "    docutils-0.16              |           py36_1         669 KB\n",
      "    jupyterlab-2.2.6           |             py_0         3.4 MB\n",
      "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
      "    jupyterlab_server-1.2.0    |             py_0          25 KB\n",
      "    keras-2.4.3                |                0          12 KB\n",
      "    keras-base-2.4.3           |             py_0          37 KB\n",
      "    nb_conda_kernels-2.3.1     |   py36h06a4308_0          28 KB\n",
      "    nbclient-0.5.1             |             py_0          58 KB\n",
      "    nbconvert-6.0.7            |           py36_0         480 KB\n",
      "    nest-asyncio-1.4.3         |     pyhd3eb1b0_0          11 KB\n",
      "    notebook-6.1.4             |           py36_0         4.1 MB\n",
      "    numpydoc-1.1.0             |     pyhd3eb1b0_1          42 KB\n",
      "    pylint-2.6.0               |           py36_0         443 KB\n",
      "    python-language-server-0.36.1|     pyhd3eb1b0_1          41 KB\n",
      "    sphinx-3.2.1               |             py_0         1.1 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  argon2-cffi        pkgs/main/linux-64::argon2-cffi-20.1.0-py36h7b6447c_1\n",
      "  async_generator    pkgs/main/linux-64::async_generator-1.10-py36h28b3542_0\n",
      "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py36h27cfd23_1003\n",
      "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0\n",
      "  keras-base         pkgs/main/noarch::keras-base-2.4.3-py_0\n",
      "  nbclient           pkgs/main/noarch::nbclient-0.5.1-py_0\n",
      "  nest-asyncio       pkgs/main/noarch::nest-asyncio-1.4.3-pyhd3eb1b0_0\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  binutils_impl_linux-64-2.33.1-he6710b0_7\n",
      "  binutils_linux-64-2.33.1-h9595d00_15\n",
      "  gcc_impl_linux-64-7.3.0-habb00fd_1\n",
      "  gcc_linux-64-7.3.0-h553295d_15\n",
      "  gxx_impl_linux-64-7.3.0-hdf63c60_1\n",
      "  gxx_linux-64-7.3.0-h553295d_15\n",
      "  keras-applications-1.0.8-py_1\n",
      "  libgpuarray-0.7.6-h14c3975_0\n",
      "  mako-1.1.3-py_0\n",
      "  pygpu-0.7.6-py36heb32a55_0\n",
      "  theano-1.0.4-py36hfd86e86_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  astroid                                      2.3.3-py36_0 --> 2.4.2-py36_0\n",
      "  bleach             pkgs/main/linux-64::bleach-3.1.0-py36~ --> pkgs/main/noarch::bleach-3.2.1-py_0\n",
      "  docutils                                      0.16-py36_0 --> 0.16-py36_1\n",
      "  isort              pkgs/main/linux-64::isort-4.3.21-py36~ --> pkgs/main/noarch::isort-5.6.4-py_0\n",
      "  jupyterlab                             1.2.6-pyhf63ae98_0 --> 2.2.6-py_0\n",
      "  jupyterlab_server                              1.0.6-py_0 --> 1.2.0-py_0\n",
      "  keras              conda-forge/linux-64::keras-2.3.1-py3~ --> pkgs/main/noarch::keras-2.4.3-0\n",
      "  nb_conda_kernels                             2.2.4-py36_0 --> 2.3.1-py36h06a4308_0\n",
      "  nbconvert                                    5.6.1-py36_0 --> 6.0.7-py36_0\n",
      "  notebook                                     6.0.3-py36_0 --> 6.1.4-py36_0\n",
      "  numpydoc                                       0.9.2-py_0 --> 1.1.0-pyhd3eb1b0_1\n",
      "  pip                                         20.0.2-py36_1 --> 20.3.1-py36h06a4308_0\n",
      "  pylint                                       2.4.4-py36_0 --> 2.6.0-py36_0\n",
      "  python-language-s~                            0.35.1-py_0 --> 0.36.1-pyhd3eb1b0_1\n",
      "  sphinx                                         2.4.0-py_0 --> 3.2.1-py_0\n",
      "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py~ --> pkgs/main/noarch::urllib3-1.25.11-py_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  lazy-object-proxy                    1.5.2-py36h27cfd23_0 --> 1.4.3-py36h7b6447c_0\n",
      "  wrapt                               1.12.1-py36h7b6447c_1 --> 1.11.2-py36h7b6447c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "pylint-2.6.0         | 443 KB    | ##################################### | 100% \n",
      "nbclient-0.5.1       | 58 KB     | ##################################### | 100% \n",
      "argon2-cffi-20.1.0   | 46 KB     | ##################################### | 100% \n",
      "nb_conda_kernels-2.3 | 28 KB     | ##################################### | 100% \n",
      "nest-asyncio-1.4.3   | 11 KB     | ##################################### | 100% \n",
      "async_generator-1.10 | 39 KB     | ##################################### | 100% \n",
      "astroid-2.4.2        | 279 KB    | ##################################### | 100% \n",
      "sphinx-3.2.1         | 1.1 MB    | ##################################### | 100% \n",
      "keras-base-2.4.3     | 37 KB     | ##################################### | 100% \n",
      "python-language-serv | 41 KB     | ##################################### | 100% \n",
      "docutils-0.16        | 669 KB    | ##################################### | 100% \n",
      "jupyterlab_pygments- | 8 KB      | ##################################### | 100% \n",
      "keras-2.4.3          | 12 KB     | ##################################### | 100% \n",
      "jupyterlab_server-1. | 25 KB     | ##################################### | 100% \n",
      "numpydoc-1.1.0       | 42 KB     | ##################################### | 100% \n",
      "nbconvert-6.0.7      | 480 KB    | ##################################### | 100% \n",
      "jupyterlab-2.2.6     | 3.4 MB    | ##################################### | 100% \n",
      "notebook-6.1.4       | 4.1 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: | b'Disabling nb_conda_kernels...\\nStatus: disabled\\n'\n",
      "| b'Enabling nb_conda_kernels...\\nCONDA_PREFIX: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\\nStatus: enabled\\n'\n",
      "done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - imagehash\n",
      "    - pandas-profiling\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py36h5fab9bb_0         143 KB  conda-forge\n",
      "    confuse-1.4.0              |     pyhd3deb0d_0          21 KB  conda-forge\n",
      "    htmlmin-0.1.12             |             py_1          21 KB  conda-forge\n",
      "    imagehash-4.2.0            |     pyhd8ed1ab_0         292 KB  conda-forge\n",
      "    missingno-0.4.2            |             py_1          12 KB  conda-forge\n",
      "    pandas-profiling-2.9.0     |     pyh9f0ad1d_0         176 KB  conda-forge\n",
      "    phik-0.10.0                |             py_0         578 KB  conda-forge\n",
      "    seaborn-base-0.11.0        |     pyhd8ed1ab_1         216 KB  conda-forge\n",
      "    tangled-up-in-unicode-0.0.6|     pyh9f0ad1d_0         2.0 MB  conda-forge\n",
      "    visions-0.5.0              |     pyh9f0ad1d_0          42 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  confuse            conda-forge/noarch::confuse-1.4.0-pyhd3deb0d_0\n",
      "  htmlmin            conda-forge/noarch::htmlmin-0.1.12-py_1\n",
      "  imagehash          conda-forge/noarch::imagehash-4.2.0-pyhd8ed1ab_0\n",
      "  missingno          conda-forge/noarch::missingno-0.4.2-py_1\n",
      "  pandas-profiling   conda-forge/noarch::pandas-profiling-2.9.0-pyh9f0ad1d_0\n",
      "  phik               conda-forge/noarch::phik-0.10.0-py_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "  seaborn-base       conda-forge/noarch::seaborn-base-0.11.0-pyhd8ed1ab_1\n",
      "  tangled-up-in-uni~ conda-forge/noarch::tangled-up-in-unicode-0.0.6-pyh9f0ad1d_0\n",
      "  visions            conda-forge/noarch::visions-0.5.0-pyh9f0ad1d_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2020.12.8-~ --> conda-forge::ca-certificates-2020.12.5-ha878542_0\n",
      "  certifi            pkgs/main::certifi-2020.12.5-py36h06a~ --> conda-forge::certifi-2020.12.5-py36h5fab9bb_0\n",
      "  openssl              pkgs/main::openssl-1.1.1i-h27cfd23_0 --> conda-forge::openssl-1.1.1h-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missingno-0.4.2      | 12 KB     | ##################################### | 100% \n",
      "tangled-up-in-unicod | 2.0 MB    | ##################################### | 100% \n",
      "imagehash-4.2.0      | 292 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "htmlmin-0.1.12       | 21 KB     | ##################################### | 100% \n",
      "pandas-profiling-2.9 | 176 KB    | ##################################### | 100% \n",
      "seaborn-base-0.11.0  | 216 KB    | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "visions-0.5.0        | 42 KB     | ##################################### | 100% \n",
      "phik-0.10.0          | 578 KB    | ##################################### | 100% \n",
      "confuse-1.4.0        | 21 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - ipywidgets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    confuse-1.4.0              |     pyhd3eb1b0_0          22 KB\n",
      "    htmlmin-0.1.12             |     pyhd3eb1b0_1          24 KB\n",
      "    missingno-0.4.2            |     pyhd3eb1b0_1          15 KB\n",
      "    phik-0.10.0                |     pyhd3eb1b0_0         535 KB\n",
      "    tangled-up-in-unicode-0.0.6|     pyhd3eb1b0_0         699 KB\n",
      "    visions-0.5.0              |     pyhd3eb1b0_0          45 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.3 MB\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  python_abi-3.6-1_cp36m\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2020.12.8-h06a4308_0\n",
      "  openssl            conda-forge::openssl-1.1.1h-h516909a_0 --> pkgs/main::openssl-1.1.1i-h27cfd23_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge::certifi-2020.12.5-py36h5~ --> pkgs/main::certifi-2020.12.5-py36h06a4308_0\n",
      "  confuse            conda-forge::confuse-1.4.0-pyhd3deb0d~ --> pkgs/main::confuse-1.4.0-pyhd3eb1b0_0\n",
      "  htmlmin                  conda-forge::htmlmin-0.1.12-py_1 --> pkgs/main::htmlmin-0.1.12-pyhd3eb1b0_1\n",
      "  missingno               conda-forge::missingno-0.4.2-py_1 --> pkgs/main::missingno-0.4.2-pyhd3eb1b0_1\n",
      "  phik                        conda-forge::phik-0.10.0-py_0 --> pkgs/main::phik-0.10.0-pyhd3eb1b0_0\n",
      "  tangled-up-in-uni~ conda-forge::tangled-up-in-unicode-0.~ --> pkgs/main::tangled-up-in-unicode-0.0.6-pyhd3eb1b0_0\n",
      "  visions            conda-forge::visions-0.5.0-pyh9f0ad1d~ --> pkgs/main::visions-0.5.0-pyhd3eb1b0_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "htmlmin-0.1.12       | 24 KB     | ##################################### | 100% \n",
      "phik-0.10.0          | 535 KB    | ##################################### | 100% \n",
      "visions-0.5.0        | 45 KB     | ##################################### | 100% \n",
      "missingno-0.4.2      | 15 KB     | ##################################### | 100% \n",
      "confuse-1.4.0        | 22 KB     | ##################################### | 100% \n",
      "tangled-up-in-unicod | 699 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This cell may take 15-20 mins to finish. If you don'y need Pandas Profiler data analysis, just skip this cell,\n",
    "# as well as `a. Automatic Data Exploration` section\n",
    "\n",
    "# System cell\n",
    "# Sophisticated way to import pandas profiler into the Notebook. Work with kernel = 'conda_amazonei_tensorflow_p36\n",
    "try:\n",
    "    import pandas_profiling\n",
    "except:\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 --all -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda install -c conda-forge -n amazonei_tensorflow_p36 pandas-profiling imagehash -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 ipywidgets -y\n",
    "finally:\n",
    "    import pandas_profiling\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import required and common libs\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pyarrow\n",
    "from itables import show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - shap\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    shap-0.36.0                |   py36h7c3b610_0         492 KB  conda-forge\n",
      "    slicer-0.0.3               |             py_0          14 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         506 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "  shap               conda-forge/linux-64::shap-0.36.0-py36h7c3b610_0\n",
      "  slicer             conda-forge/noarch::slicer-0.0.3-py_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "shap-0.36.0          | 492 KB    | ##################################### | 100% \n",
      "slicer-0.0.3         | 14 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_to_object_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-6fd4a145a53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install -y -c defaults -c conda-forge shap # > /dev/null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kernel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKernel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKernelExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSampling\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSamplingExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/explainers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_permutation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_partition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_exact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_additive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdditive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_import_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaskers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/utils/_legacy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spectral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from ._mean_shift import (mean_shift, MeanShift,\n\u001b[1;32m      8\u001b[0m                           estimate_bandwidth, get_bin_seeds)\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/cluster/_spectral.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kmeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/neighbors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/neighbors/_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# License: BSD 3 clause (C) INRIA, University of Amsterdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnsupervisedMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_to_object_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_to_object_array'"
     ]
    }
   ],
   "source": [
    "!conda install -y -c defaults -c conda-forge shap # > /dev/null\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import all required MLFactory classes, and init MLFactory API\n",
    "from mlfactory.sdk.restclient.mlfactory_api import MLFactoryApi\n",
    "from mlfactory.sdk.common import Util\n",
    "from mlfactory.sdk.problem.base import Problem\n",
    "from mlfactory.sdk.tf.transformations import Tfs\n",
    "from mlfactory.sdk.common import ExperimentLoader\n",
    "from mlfactory.sdk.tf.tf_execution import TfExecution\n",
    "\n",
    "\n",
    "mlf_api = MLFactoryApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_to_object_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8c9004a625ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafe_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAFEConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainabilityConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability_controller\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainabilityController\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/mlfactory/sdk/explainability/explainability_controller.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdpbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kernel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKernel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mKernelExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSampling\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSamplingExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTreeExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/explainers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_permutation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_partition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_exact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_additive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdditive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massert_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_import_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_isinstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmaskers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/shap/utils/_legacy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spectral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from ._mean_shift import (mean_shift, MeanShift,\n\u001b[1;32m      8\u001b[0m                           estimate_bandwidth, get_bin_seeds)\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/cluster/_spectral.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kmeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/neighbors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_unsupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/neighbors/_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# License: BSD 3 clause (C) INRIA, University of Amsterdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRadiusNeighborsMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeighborsBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnsupervisedMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_to_object_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_to_object_array'"
     ]
    }
   ],
   "source": [
    "#Predict specific imports\n",
    "import hiplot as hip\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.pyplot import xticks\n",
    "%matplotlib inline\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "from mlfactory.sdk.predict.predict_config import PredictConfiguration\n",
    "from mlfactory.sdk.aws_autopilot.aws_autopilot_config import AWSAutopilotConfiguration\n",
    "from mlfactory.sdk.afe.afe_config import AFEConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_config import ExplainabilityConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_controller import ExplainabilityController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"id\": 200136,\n",
       "    \"name\": \"Demo\",\n",
       "    \"type\": \"predict\",\n",
       "    \"transformed_data_format\": \"CSV\",\n",
       "    \"metadata\": {},\n",
       "    \"exploration_notebook_arn\": \"arn:aws:sagemaker:us-east-1:082830052325:notebook-instance/ml-factory-exploration-notebook-200136\",\n",
       "    \"transformed_data_s3_location\": \"s3://cn-ml-factory-data-bucket-zt9tmi93zfp0v3q/transformed-data/200136\",\n",
       "    \"aws_notebook_instance_type\": \"ml.t3.medium\",\n",
       "    \"github_url\": \"https://github.com/trilogy-group/cn-ml-factory-Problem-200136\",\n",
       "    \"github_username\": null,\n",
       "    \"data_source\": 91,\n",
       "    \"creator\": 24,\n",
       "    \"creator_email\": \"sanket.agarwal@codenation.co.in\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo Output JSON as table\n",
    "problem = Problem.load(problem_id)\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of tables in this problem\n",
    "tables = problem.table_names()\n",
    "tables=['train_csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to predict\n",
    "# problem.columns(<table_name>)\n",
    "target_column = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the place to explore & visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid_data_csv', 'train_csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table names under your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Automatic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and Analize your data automatically based on Pandas Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to the table name you want to explore\n",
    "# Generally, you'd like to see here your main (targe time series) dataset\n",
    "explore_table_name = \"train_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot import name 's3'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-d324ec798e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data frame from the exloration table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_explore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_dataframe_from_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplore_table_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_explore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# Data frame from the exloration table\n",
    "df_explore = problem.read_dataframe_from_table(explore_table_name)\n",
    "df_explore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can not describe a `lazy` ProfileReport without a DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e9046b74c9d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Automatic data exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_explore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Exploration report for {explore_table_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplorative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_widgets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mto_widgets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidgets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_repr_html_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mwidgets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_widgets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_widgets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_widgets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_widgets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36m_render_widgets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas_profiling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflavours\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWidgetReport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mdisable_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"progress_bar\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mreport\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_report_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas_profiling/profile_report.py\u001b[0m in \u001b[0;36mdescription_set\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescription_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_description_set\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_description_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescribe_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_description_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/pandas_profiling/model/describe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(title, df, sample)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not describe a `lazy` ProfileReport without a DataFrame.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can not describe a `lazy` ProfileReport without a DataFrame."
     ]
    }
   ],
   "source": [
    "# System Cell\n",
    "# Automatic data exploration\n",
    "profile = ProfileReport(df_explore, title=f\"Exploration report for {explore_table_name}\", explorative=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Automatic Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect outliers in your data using MLFactory AnomalyDetection transformation\n",
    "\n",
    "[Read details](https://docs.google.com/document/d/1xyV_paZdy3vW9S954korzOqmZVDX56Yavu2OA0zKT_Y/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        CAUTION: This transformation should be APPLIED AFTER train/test SPLIT on training data to PREVENT DATA LEAKAGE.\n",
      " CAUTION: This transformation requires the DATA TO BE NUMERICAL in nature, i.e. the data passed by the user should have all numerical values (Dtype of all columns should be numeric) otherwise the Transformation will FAIL.\n",
      " Adds is_outlier column where 1 defines an anomaly. User can then manipulate data based on this.\n",
      "    \n",
      "        Required properties: table, name, problem_id\n",
      "    \n",
      "        Attributes: \n",
      "            table    Table. Type = 'string'.  \n",
      "            votes    Number of votes(1-8) to be considered an anomaly. By default 3. Type = 'number'.  \n",
      "            outliers_fraction    Estimated proportion of outliers in the data set (float in (0., 0.5)). By default 0.1. Type = 'number'.  \n",
      "            ignoreHeader    Ignore CSV Header while saving in s3. Type = 'boolean'.  \n",
      "            partitions    Partitions. Type = 'number'. Set number of partitions of output table. \n",
      "    Leave empty if don\"t want to specify partitions. \n",
      "            name    Transformation name. Type = 'string'. The name of the output table. Please ensure that you assign transformation names which are unique in the scope of the current problem. Default = ''anomaly_detection''.\n",
      "            problem_id    Problem Id. Type = 'number'. Id of the problem your are working on. \n",
      "    \n",
      "        Example: \n",
      "            tf.table = \"table1\"\n",
      "            tf.votes = 6\n",
      "            tf.outliers_fraction = 0.2\n",
      "            tf.name = \"my_first_anomaly_detection_transformation\"\n",
      "            tf.problem_id = 147\n",
      "        \n",
      "Transformation saved, id is 937\n",
      "Running AnomalyDetection. Transformation id is 937\n",
      "Execution started. Result will be available in table 'anomaly_detection'. \n",
      "For current execution status, run `tf.tf_execution.refresh_status()`\n",
      "Execution workflow link: https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=workflows;workflowView=workflow-run-details?runId=None&workflowName=\n",
      "Logs link: https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/ml-factory/glue-logs;stream=ml_factory_transformation_200136_937_7lkz5y7nc2\n",
      "\n",
      "Please check your email for 'AWS Notifications' message and confirm subscription, to receive notification when the transformation is done\n",
      "In case you don't see it, most probably your email 'sanket.agarwal@codenation.co.in' is already confirmed for that datasource\n",
      "Please be aware that sometimes, the confirmation email  falls into SPAM folder.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"id\": 937,\n",
       "    \"status\": \"STARTING_TRANSFORMATION\",\n",
       "    \"start_time\": \"2020-12-10 15:07:53.823974\",\n",
       "    \"end_time\": null,\n",
       "    \"glue_workflow_name\": \"\",\n",
       "    \"glue_workflow_run_id\": null,\n",
       "    \"cloudwatch_stream_name\": \"ml_factory_transformation_200136_937_7lkz5y7nc2\",\n",
       "    \"transformation\": {\n",
       "      \"save\": true,\n",
       "      \"type\": \"anomaly_detection\",\n",
       "      \"name\": \"anomaly_detection_problem_200136\",\n",
       "      \"config\": {\n",
       "        \"name\": \"anomaly_detection\",\n",
       "        \"problem_id\": 200136,\n",
       "        \"table\": \"train_csv\",\n",
       "        \"ignoreHeader\": false\n",
       "      }\n",
       "    },\n",
       "    \"problem\": {\n",
       "      \"id\": 200136,\n",
       "      \"name\": \"Demo\",\n",
       "      \"type\": \"predict\",\n",
       "      \"transformed_data_format\": \"CSV\",\n",
       "      \"metadata\": {},\n",
       "      \"exploration_notebook_arn\": \"arn:aws:sagemaker:us-east-1:082830052325:notebook-instance/ml-factory-exploration-notebook-200136\",\n",
       "      \"transformed_data_s3_location\": \"s3://cn-ml-factory-data-bucket-zt9tmi93zfp0v3q/transformed-data/200136\",\n",
       "      \"aws_notebook_instance_type\": \"ml.t3.medium\",\n",
       "      \"github_url\": \"https://github.com/trilogy-group/cn-ml-factory-Problem-200136\",\n",
       "      \"github_username\": null,\n",
       "      \"data_source\": 91,\n",
       "      \"creator\": 24,\n",
       "      \"creator_email\": \"sanket.agarwal@codenation.co.in\"\n",
       "    },\n",
       "    \"artefact_location\": null\n",
       "  },\n",
       "  \"problem_id\": 200136,\n",
       "  \"transformation_id\": 937\n",
       "}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run this cell if you want to apply automatic anomalies detecion\n",
    "\n",
    "anomaly_detector = Tfs.AnomalyDetection()\n",
    "anomaly_detector.problem_id = problem.id()\n",
    "anomaly_detector.table = explore_table_name\n",
    "anomaly_detector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies detection is still in progress, please wait. Current status is TfExecutionStatus.STARTING_TRANSFORMATION\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell if you appled automatic anomalies detecion above.\n",
    "\n",
    "status = anomaly_detector.tf_execution.refresh_status()\n",
    "if status.is_done():\n",
    "    df_explore = problem.read_dataframe_from_table(anomaly_detector.name)\n",
    "    df_explore.head()\n",
    "else:\n",
    "    print(f\"Anomalies detection is still in progress, please wait. Current status is {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Custom Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to explore more data if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory and Server-Side Transformations Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two options to transform your data:\n",
    " 1. Use `problem.read_dataframe_from_table(<table_name>)` to load all your data in memory into good old pandas DataFrame.\n",
    "  Apply all transformations your need, and save data using `problem.write_dataframe_into_table(<df>, <table_name>)`.\n",
    "  That suits perfectly if you have a small to medium size dataset which fits to RAM (up to several Gbs)\n",
    " 2. Use server-side MLFactory SDK transformations. They run at our backend, take more time (minutes usually),\n",
    "  but can handle, and are intended for, huge datasets.\n",
    "\n",
    "Please find examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations have been divided into 3 sections:\n",
    "1. Transformations before Train Test Split\n",
    "2. Train Test Split\n",
    "3. Transformations after Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server-side transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choose and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. To list all available server-side transformations, invoke \"all()\"\n",
    "\n",
    "# Tfs.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Choose one, e.g. `SelectColumns\n",
    "\n",
    "# tf = Tfs.SelectColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. To get help on a transformation (as well as almost any SDK class or function), invoke `?`,\n",
    "# or put cursor on variable or function name and Shift+Tab\n",
    "\n",
    "# tf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Configure the transformation\n",
    "# Name of the transformation is also used as the name of output table you will find transformed data in\n",
    "# By default, the name is assigned to the name of transformation itself, in snake case\n",
    "\n",
    "\n",
    "# tf.name = \"select_best_columns\"\n",
    "\n",
    "# tf.table = \"raw_data_csv\"\n",
    "\n",
    "# tf.keys = [\"item_id\"]\n",
    "\n",
    "# tf.problem_id = problem.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution = tf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Check the transformation execution status\n",
    "# tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should wait for (usually) several minutes. You can use those flags to understand if execution is complete and if it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution.refresh_status().is_done()\n",
    "# tf_execution.refresh_status().is_successful()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic (Pandas) transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid_data_csv', 'train_csv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All table names for your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot import name 's3'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6e4c1064640e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read data into pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_dataframe_from_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# Read data into pandas DataFrame\n",
    "df = problem.read_dataframe_from_table('train_csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Change Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do changes using pandas\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Save data back to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check available parameters running the line below\n",
    "# problem.write_dataframe_into_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data\n",
    "# problem.write_dataframe_into_table(df, table_name=<new table name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to Required Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Last Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. If you use server-side transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-af0195ed9c9d>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-af0195ed9c9d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tf_final = Tfs.<SELECT CLASS>()\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Select transformation class from Tfs.all() list, and replace <SELECT CLASS> with class name\n",
    "tf_final = Tfs.<SELECT CLASS>()\n",
    "# Last transformation in the chain should have the predefined name = \"target_time_series\"\n",
    "# That's required by our backend: we don't specify time series table name explicitly but rather expect data to be\n",
    "# present in a table named \"target_time_series\"\n",
    "tf_final.name = \"target_time_series\"\n",
    "tf_final.partitions = 1\n",
    "tf_final.problem_id = problem.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3abc03e93799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtf_execution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_final' is not defined"
     ]
    }
   ],
   "source": [
    "# System cell\n",
    "# Long running\n",
    "\n",
    "# Run transformation\n",
    "tf_execution = tf_final.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_execution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-003c14ded11d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Wait until `is_done` is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtf_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf_execution' is not defined"
     ]
    }
   ],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Wait until `is_done` is True\n",
    "tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Or, If you use in-memory pandas transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non of the following tables found in datasource id = 91, name = demo2: _parquet_ml_factory_ingestion_91_t1_problem_120_problem_200136, parquet_ml_factory_ingestion_91_t1_problem_120_problem_200136, _parquet_ml_factory_ingestion_91_t1_problem_120, parquet_ml_factory_ingestion_91_t1_problem_120\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0d372b55262b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_dataframe_from_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't1_problem_120'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Do changes using pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df = problem.read_dataframe_from_table('t1_problem_120')\n",
    "df.head()\n",
    "# Do changes using pandas\n",
    "# ...\n",
    "# Write date\n",
    "# problem.write_dataframe_into_table(df, \"target_time_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations BEFORE Train Test Split\n",
    "\n",
    "Usually Transformations which add features or modify existing features into new ones can be applied before Train Test Split.\n",
    "\n",
    "For example:\n",
    "- Absolute - Computes the absolute value of a number.\n",
    "- SelectColumns - Selects specified columns of interest from the table.\n",
    "\n",
    "Can be applied BEFORE the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations BEFORE splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Forms Train Validation and Test tables from the given table by randomly shuffling the rows and splitting up the table according to the defined percentages.\n",
      "    \n",
      "        Required properties: table, name, problem_id\n",
      "    \n",
      "        Attributes: \n",
      "            table    Table. Type = 'string'.  \n",
      "            p    probability array to split table. Type = 'array'.  Default = '[0.65, 0.15, 0.2]'.\n",
      "            ignoreHeader    Ignore CSV Header while saving in s3. Type = 'boolean'.  \n",
      "            partitions    Partitions. Type = 'number'. Set number of partitions of output table. \n",
      "    Leave empty if don\"t want to specify partitions. \n",
      "            name    Transformation name. Type = 'string'. The name of the output table. Please ensure that you assign transformation names which are unique in the scope of the current problem. Default = ''split''.\n",
      "            problem_id    Problem Id. Type = 'number'. Id of the problem your are working on. \n",
      "    \n",
      "        Example: \n",
      "            tf.table = \"Opportunity\"\n",
      "            tf.p = \"[0.6, 0.2, 0.2] # default [0.65, 0.15, 0.20]\"\n",
      "            tf.name = \"my_first_split_transformation\"\n",
      "            tf.problem_id = 147\n",
      "        \n",
      "'table' property is required, please set its value.\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment and run this cell to apply train validation test split\n",
    "\n",
    "train_val_test_splitter = Tfs.Split()\n",
    "train_val_test_splitter.name = \"\"\n",
    "train_val_test_splitter.ignoreHeader = True\n",
    "train_val_test_splitter.table = \"\"\n",
    "train_val_test_splitter.p = [0.65, 0.15, 0.20]\n",
    "train_val_test_splitter.problem_id = problem.id()\n",
    "train_val_test_splitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations AFTER Train Test Split\n",
    "\n",
    "Some Transformations should be applied after the Train Test Split separately as otherwise they can lead to Leakage of Data.\n",
    "\n",
    "These usually include Encoder, Scaler, Normalizer Transformations.\n",
    "\n",
    "For example:\n",
    "- OneHotEncode - Transforms the chosen columns into one-hot representations.\n",
    "- AnomalyDetection - Adds is_outlier column where 1 defines an anomaly\n",
    "\n",
    "Should be applied AFTER the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations AFTER splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "experiment = PredictConfiguration.load(problem_id=problem_id, experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"data\": {\n",
       "      \"experiment\": {\n",
       "        \"id\": 650,\n",
       "        \"transformations\": \"\",\n",
       "        \"name\": \"Demo\",\n",
       "        \"description\": \"demo\",\n",
       "        \"problem_type\": \"Predict\",\n",
       "        \"comparison_metric\": \"rmse\",\n",
       "        \"comparison_metric_optimization_type\": \"Maximize\",\n",
       "        \"custom_fields\": {\n",
       "          \"learning_objective\": \"BinaryClassification\",\n",
       "          \"framework\": \"xgboost\",\n",
       "          \"feature_engineering\": \"AFE\",\n",
       "          \"data_enrichment\": \"no\",\n",
       "          \"explainability\": \"yes\",\n",
       "          \"metric_definitions\": [],\n",
       "          \"positive_data_weight\": 1,\n",
       "          \"sagemaker_experiment_name\": \"Demo-1607510130\"\n",
       "        },\n",
       "        \"dataset\": {},\n",
       "        \"neptune_project_name\": \"cn-ml-factory\",\n",
       "        \"email\": \"cn_ml_factory_service_user@codenation.co.in\",\n",
       "        \"create_time\": \"2020-12-09T10:35:31.024104Z\",\n",
       "        \"last_updated_time\": \"2020-12-09T10:35:32.216530Z\",\n",
       "        \"status\": \"Running Trials\",\n",
       "        \"reason\": \"\",\n",
       "        \"notebook_link\": \"https://ml-factory-exploration-notebook-200136.notebook.us-east-1.sagemaker.aws/notebooks/cn-ml-factory-Problem-200136/ml-factory-exploration-notebook-200136/SageMaker/Demo.ipynb\",\n",
       "        \"problem\": 200136,\n",
       "        \"neptune_project_link\": \"https://ui.neptune.ai/cn-ml-factory-user/cn-ml-factory\",\n",
       "        \"xray_link\": \"https://console.aws.amazon.com/xray/home?region=us-east-1#/traces?timeRange=PT5M\",\n",
       "        \"best_trial_deployment_data\": null,\n",
       "        \"best_trial_triggered_model_data\": null,\n",
       "        \"best_trial_metrics\": null,\n",
       "        \"problem_id\": 200136\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  \"name\": \"Demo\",\n",
       "  \"description\": \"demo\",\n",
       "  \"neptune_project_name\": \"cn-ml-factory\",\n",
       "  \"training_data_s3_uri\": null,\n",
       "  \"validation_data_s3_uri\": null,\n",
       "  \"test_data_s3_uri\": null,\n",
       "  \"learning_objective\": null,\n",
       "  \"comparison_metric\": null,\n",
       "  \"custom_fields\": null,\n",
       "  \"positive_data_weight\": 1.0,\n",
       "  \"metric_definitions\": [],\n",
       "  \"hpt_max_jobs\": 5,\n",
       "  \"hpt_max_parallel_jobs\": 5,\n",
       "  \"train_instance_count\": 1,\n",
       "  \"train_instance_type\": \"ml.m4.xlarge\",\n",
       "  \"test_instance_count\": 1,\n",
       "  \"test_instance_type\": \"ml.m4.xlarge\",\n",
       "  \"data_enrichers\": null\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Fields (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those fields are mandatory, and require you to set values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to [the XGBoost Documentation](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters) for all possible `learning_objective`s and `comparison_metric`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please give your experiment a meaninful name\n",
    "# It should be unique in the scope of the problem\n",
    "experiment.name = \"Demo\"\n",
    "experiment.description = \"demo\"\n",
    "\n",
    "#specify the dataset for the experiment\n",
    "# Format is : <tr/ain_test_split_transformation_name>_problem_<problem_id>_<i>\n",
    "# i=0 for train data, i=1 for valid data and i=2 for test data\n",
    "\n",
    "experiment.training_data_s3_uri = \"\"\n",
    "experiment.validation_data_s3_uri = \"\"\n",
    "experiment.test_data_s3_uri = \"\"\n",
    "\n",
    "# Valid Pairs include\n",
    "# binary:logistic -> error  | auc | aucpr\n",
    "# multi:softprob  -> merror | auc | aucpr\n",
    "# reg:logistic    -> rmse  | rmsle | mae | mape\n",
    "experiment.learning_objective = \"BinaryClassification\"\n",
    "experiment.comparison_metric = \"rmse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can leave the values as is, or adjust them if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment and change any of these default values\n",
    "\n",
    "experiment.positive_data_weight: float = 1.0\n",
    "experiment.hpt_max_jobs: int = 5\n",
    "experiment.hpt_max_parallel_jobs: int = 5\n",
    "experiment.train_instance_count: int = 1\n",
    "experiment.train_instance_type: str = \"ml.m4.xlarge\"\n",
    "experiment.test_instance_count: int = 1\n",
    "experiment.test_instance_type: str = \"ml.m4.xlarge\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running training make sure\n",
    "  - first column contains the target value  [use : MoveColumnToFirstGlueTransformer]\n",
    "  - the dataset headers are extracted and uploaded to S3 [use : UpdateColumnsGlueTransformer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved. Id is None\n",
      "Running training for experiment id = 652\n",
      "Bad Request\n"
     ]
    }
   ],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "experiment.save_and_run_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now all you need is to wait until training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-df3f541e0f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Training could take several hours, or even more, on huge datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperimentLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/mlfactory/sdk/experiment/base.py\u001b[0m in \u001b[0;36mstatus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mintercept_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# When the status = 'Training complete', we can proceed further. Otherwise, we need to wait.\n",
    "# Training could take several hours, or even more, on huge datasets\n",
    "experiment = ExperimentLoader.load(problem_id, experiment_id)\n",
    "experiment.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find the model valuable, you might consider deploying it to reuse in the future.\n",
    "Uncomment and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "\n",
    "# experiment.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# When deployment is done, you can run inferences using the inference link\n",
    "# experiment.inference_link()\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda_amazonei_tensorflow_p36"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

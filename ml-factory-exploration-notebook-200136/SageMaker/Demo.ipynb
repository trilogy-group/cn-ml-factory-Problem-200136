{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
       "// on document structure change\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
    "// on document structure change\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764\n"
     ]
    }
   ],
   "source": [
    "# Parameter: URL of the REST API\n",
    "%env MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Problem is created in UI, here we use its ID as parameter\n",
    "problem_id = 200136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Experiment is pre-created in UI, here we use its ID as parameter to load and reuse it\n",
    "experiment_id = 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install MLFactory SDK\n",
    "!pip install mlfactory_sdk --upgrade --extra-index https://pypi.swarm.devfactory.com > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    argon2-cffi-20.1.0         |   py36h7b6447c_1          46 KB\n",
      "    astroid-2.4.2              |           py36_0         279 KB\n",
      "    async_generator-1.10       |   py36h28b3542_0          39 KB\n",
      "    docutils-0.16              |           py36_1         669 KB\n",
      "    jupyterlab-2.2.6           |             py_0         3.4 MB\n",
      "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
      "    jupyterlab_server-1.2.0    |             py_0          25 KB\n",
      "    keras-2.4.3                |                0          12 KB\n",
      "    keras-base-2.4.3           |             py_0          37 KB\n",
      "    nb_conda_kernels-2.3.1     |   py36h06a4308_0          28 KB\n",
      "    nbclient-0.5.1             |             py_0          58 KB\n",
      "    nbconvert-6.0.7            |           py36_0         480 KB\n",
      "    nest-asyncio-1.4.3         |     pyhd3eb1b0_0          11 KB\n",
      "    notebook-6.1.4             |           py36_0         4.1 MB\n",
      "    numpydoc-1.1.0             |     pyhd3eb1b0_1          42 KB\n",
      "    pylint-2.6.0               |           py36_0         443 KB\n",
      "    python-language-server-0.36.1|     pyhd3eb1b0_1          41 KB\n",
      "    sphinx-3.2.1               |             py_0         1.1 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  argon2-cffi        pkgs/main/linux-64::argon2-cffi-20.1.0-py36h7b6447c_1\n",
      "  async_generator    pkgs/main/linux-64::async_generator-1.10-py36h28b3542_0\n",
      "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py36h27cfd23_1003\n",
      "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0\n",
      "  keras-base         pkgs/main/noarch::keras-base-2.4.3-py_0\n",
      "  nbclient           pkgs/main/noarch::nbclient-0.5.1-py_0\n",
      "  nest-asyncio       pkgs/main/noarch::nest-asyncio-1.4.3-pyhd3eb1b0_0\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  binutils_impl_linux-64-2.33.1-he6710b0_7\n",
      "  binutils_linux-64-2.33.1-h9595d00_15\n",
      "  gcc_impl_linux-64-7.3.0-habb00fd_1\n",
      "  gcc_linux-64-7.3.0-h553295d_15\n",
      "  gxx_impl_linux-64-7.3.0-hdf63c60_1\n",
      "  gxx_linux-64-7.3.0-h553295d_15\n",
      "  keras-applications-1.0.8-py_1\n",
      "  libgpuarray-0.7.6-h14c3975_0\n",
      "  mako-1.1.3-py_0\n",
      "  pygpu-0.7.6-py36heb32a55_0\n",
      "  theano-1.0.4-py36hfd86e86_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  astroid                                      2.3.3-py36_0 --> 2.4.2-py36_0\n",
      "  bleach             pkgs/main/linux-64::bleach-3.1.0-py36~ --> pkgs/main/noarch::bleach-3.2.1-py_0\n",
      "  docutils                                      0.16-py36_0 --> 0.16-py36_1\n",
      "  isort              pkgs/main/linux-64::isort-4.3.21-py36~ --> pkgs/main/noarch::isort-5.6.4-py_0\n",
      "  jupyterlab                             1.2.6-pyhf63ae98_0 --> 2.2.6-py_0\n",
      "  jupyterlab_server                              1.0.6-py_0 --> 1.2.0-py_0\n",
      "  keras              conda-forge/linux-64::keras-2.3.1-py3~ --> pkgs/main/noarch::keras-2.4.3-0\n",
      "  nb_conda_kernels                             2.2.4-py36_0 --> 2.3.1-py36h06a4308_0\n",
      "  nbconvert                                    5.6.1-py36_0 --> 6.0.7-py36_0\n",
      "  notebook                                     6.0.3-py36_0 --> 6.1.4-py36_0\n",
      "  numpydoc                                       0.9.2-py_0 --> 1.1.0-pyhd3eb1b0_1\n",
      "  pip                                         20.0.2-py36_1 --> 20.3.1-py36h06a4308_0\n",
      "  pylint                                       2.4.4-py36_0 --> 2.6.0-py36_0\n",
      "  python-language-s~                            0.35.1-py_0 --> 0.36.1-pyhd3eb1b0_1\n",
      "  sphinx                                         2.4.0-py_0 --> 3.2.1-py_0\n",
      "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py~ --> pkgs/main/noarch::urllib3-1.25.11-py_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  lazy-object-proxy                    1.5.2-py36h27cfd23_0 --> 1.4.3-py36h7b6447c_0\n",
      "  wrapt                               1.12.1-py36h7b6447c_1 --> 1.11.2-py36h7b6447c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "sphinx-3.2.1         | 1.1 MB    | ##################################### | 100% \n",
      "pylint-2.6.0         | 443 KB    | ##################################### | 100% \n",
      "nbconvert-6.0.7      | 480 KB    | ##################################### | 100% \n",
      "keras-base-2.4.3     | 37 KB     | ##################################### | 100% \n",
      "nest-asyncio-1.4.3   | 11 KB     | ##################################### | 100% \n",
      "jupyterlab_server-1. | 25 KB     | ##################################### | 100% \n",
      "numpydoc-1.1.0       | 42 KB     | ##################################### | 100% \n",
      "nb_conda_kernels-2.3 | 28 KB     | ##################################### | 100% \n",
      "docutils-0.16        | 669 KB    | ##################################### | 100% \n",
      "async_generator-1.10 | 39 KB     | ##################################### | 100% \n",
      "argon2-cffi-20.1.0   | 46 KB     | ##################################### | 100% \n",
      "python-language-serv | 41 KB     | ##################################### | 100% \n",
      "notebook-6.1.4       | 4.1 MB    | ##################################### | 100% \n",
      "keras-2.4.3          | 12 KB     | ##################################### | 100% \n",
      "jupyterlab_pygments- | 8 KB      | ##################################### | 100% \n",
      "jupyterlab-2.2.6     | 3.4 MB    | ##################################### | 100% \n",
      "astroid-2.4.2        | 279 KB    | ##################################### | 100% \n",
      "nbclient-0.5.1       | 58 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: - b'Disabling nb_conda_kernels...\\nStatus: disabled\\n'\n",
      "| b'Enabling nb_conda_kernels...\\nCONDA_PREFIX: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\\nStatus: enabled\\n'\n",
      "done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - imagehash\n",
      "    - pandas-profiling\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py36h5fab9bb_0         143 KB  conda-forge\n",
      "    confuse-1.4.0              |     pyhd3deb0d_0          21 KB  conda-forge\n",
      "    htmlmin-0.1.12             |             py_1          21 KB  conda-forge\n",
      "    imagehash-4.2.0            |     pyhd8ed1ab_0         292 KB  conda-forge\n",
      "    missingno-0.4.2            |             py_1          12 KB  conda-forge\n",
      "    pandas-profiling-2.9.0     |     pyh9f0ad1d_0         176 KB  conda-forge\n",
      "    phik-0.10.0                |             py_0         578 KB  conda-forge\n",
      "    seaborn-base-0.11.0        |     pyhd8ed1ab_1         216 KB  conda-forge\n",
      "    tangled-up-in-unicode-0.0.6|     pyh9f0ad1d_0         2.0 MB  conda-forge\n",
      "    visions-0.5.0              |     pyh9f0ad1d_0          42 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  confuse            conda-forge/noarch::confuse-1.4.0-pyhd3deb0d_0\n",
      "  htmlmin            conda-forge/noarch::htmlmin-0.1.12-py_1\n",
      "  imagehash          conda-forge/noarch::imagehash-4.2.0-pyhd8ed1ab_0\n",
      "  missingno          conda-forge/noarch::missingno-0.4.2-py_1\n",
      "  pandas-profiling   conda-forge/noarch::pandas-profiling-2.9.0-pyh9f0ad1d_0\n",
      "  phik               conda-forge/noarch::phik-0.10.0-py_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "  seaborn-base       conda-forge/noarch::seaborn-base-0.11.0-pyhd8ed1ab_1\n",
      "  tangled-up-in-uni~ conda-forge/noarch::tangled-up-in-unicode-0.0.6-pyh9f0ad1d_0\n",
      "  visions            conda-forge/noarch::visions-0.5.0-pyh9f0ad1d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2020.10.14~ --> conda-forge::ca-certificates-2020.12.5-ha878542_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2020.12.5-py36h06a~ --> conda-forge::certifi-2020.12.5-py36h5fab9bb_0\n",
      "  openssl              pkgs/main::openssl-1.1.1h-h7b6447c_0 --> conda-forge::openssl-1.1.1h-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visions-0.5.0        | 42 KB     | ##################################### | 100% \n",
      "missingno-0.4.2      | 12 KB     | ##################################### | 100% \n",
      "imagehash-4.2.0      | 292 KB    | ##################################### | 100% \n",
      "confuse-1.4.0        | 21 KB     | ##################################### | 100% \n",
      "pandas-profiling-2.9 | 176 KB    | ##################################### | 100% \n",
      "htmlmin-0.1.12       | 21 KB     | ##################################### | 100% \n",
      "phik-0.10.0          | 578 KB    | ##################################### | 100% \n",
      "tangled-up-in-unicod | 2.0 MB    | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "seaborn-base-0.11.0  | 216 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / "
     ]
    }
   ],
   "source": [
    "# WARNING: This cell may take 15-20 mins to finish. If you don'y need Pandas Profiler data analysis, just skip this cell,\n",
    "# as well as `a. Automatic Data Exploration` section\n",
    "\n",
    "# System cell\n",
    "# Sophisticated way to import pandas profiler into the Notebook. Work with kernel = 'conda_amazonei_tensorflow_p36\n",
    "try:\n",
    "    import pandas_profiling\n",
    "except:\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 --all -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda install -c conda-forge -n amazonei_tensorflow_p36 pandas-profiling imagehash -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 ipywidgets -y\n",
    "finally:\n",
    "    import pandas_profiling\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.config({\n",
       "    paths: {\n",
       "        datatables: 'https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min',\n",
       "    }\n",
       "});\n",
       "\n",
       "$('head').append('<link rel=\"stylesheet\" type=\"text/css\" \\\n",
       "                href = \"https://cdn.datatables.net/1.10.19/css/jquery.dataTables.min.css\" > ');\n",
       "\n",
       "$('head').append('<style> table td { text-overflow: ellipsis; overflow: hidden; } </style>');\n",
       "\n",
       "$('head').append(`<script>\n",
       "function eval_functions(map_or_text) {\n",
       "    if (typeof map_or_text === \"string\") {\n",
       "        if (map_or_text.startsWith(\"function\")) {\n",
       "            try {\n",
       "                // Note: parenthesis are required around the whole expression for eval to return a value!\n",
       "                // See https://stackoverflow.com/a/7399078/911298.\n",
       "                //\n",
       "                // eval(\"local_fun = \" + map_or_text) would fail because local_fun is not declared\n",
       "                // (using var, let or const would work, but it would only be declared in the local scope\n",
       "                // and therefore the value could not be retrieved).\n",
       "                const func = eval(\"(\" + map_or_text + \")\");\n",
       "                if (typeof func !== \"function\") {\n",
       "                    // Note: backquotes are super convenient!\n",
       "                    // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals\n",
       "                    console.error(\"Evaluated expression \" + map_or_text + \" is not a function (type is \" + typeof func + \")\");\n",
       "                    return map_or_text;\n",
       "                }\n",
       "                // Return the function\n",
       "                return func;\n",
       "            } catch (e) {\n",
       "                // Make sure to print the error with a second argument to console.error().\n",
       "                console.error(\"itables was not able to parse \" + map_or_text, e);\n",
       "            }\n",
       "        }\n",
       "    } else if (typeof map_or_text === \"object\") {\n",
       "        if (map_or_text instanceof Array) {\n",
       "            // Note: \"var\" is now superseded by \"let\" and \"const\".\n",
       "            // https://medium.com/javascript-scene/javascript-es6-var-let-or-const-ba58b8dcde75\n",
       "            const result = [];\n",
       "            // Note: \"for of\" is the best way to iterate through an iterable.\n",
       "            // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/for...of\n",
       "            for (const item of map_or_text) {\n",
       "                result.push(eval_functions(item));\n",
       "            }\n",
       "            return result;\n",
       "\n",
       "            // Alternatively, more functional approach in one line:\n",
       "            // return map_or_text.map(eval_functions);\n",
       "        } else {\n",
       "            const result = {};\n",
       "            // Object.keys() is safer than \"for in\" because otherwise you might have keys\n",
       "            // that aren't defined in the object itself.\n",
       "            //\n",
       "            // See https://stackoverflow.com/a/684692/911298.\n",
       "            for (const item of Object.keys(map_or_text)) {\n",
       "                result[item] = eval_functions(map_or_text[item]);\n",
       "            }\n",
       "            return result;\n",
       "        }\n",
       "    }\n",
       "\n",
       "    return map_or_text;\n",
       "}\n",
       "</` + 'script>');"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# System cell\n",
    "# Import required and common libs\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pyarrow\n",
    "from itables import show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c defaults -c conda-forge shap # > /dev/null\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import all required MLFactory classes, and init MLFactory API\n",
    "from mlfactory.sdk.restclient.mlfactory_api import MLFactoryApi\n",
    "from mlfactory.sdk.common import Util\n",
    "from mlfactory.sdk.problem.base import Problem\n",
    "from mlfactory.sdk.tf.transformations import Tfs\n",
    "from mlfactory.sdk.common import ExperimentLoader\n",
    "from mlfactory.sdk.tf.tf_execution import TfExecution\n",
    "\n",
    "\n",
    "mlf_api = MLFactoryApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8c9004a625ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafe_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAFEConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainabilityConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainability_controller\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplainabilityController\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/mlfactory/sdk/explainability/explainability_controller.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpdpbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "#Predict specific imports\n",
    "import hiplot as hip\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.pyplot import xticks\n",
    "%matplotlib inline\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "from mlfactory.sdk.predict.predict_config import PredictConfiguration\n",
    "from mlfactory.sdk.aws_autopilot.aws_autopilot_config import AWSAutopilotConfiguration\n",
    "from mlfactory.sdk.afe.afe_config import AFEConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_config import ExplainabilityConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_controller import ExplainabilityController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"id\": 200136,\n",
       "    \"name\": \"Demo\",\n",
       "    \"type\": \"predict\",\n",
       "    \"transformed_data_format\": \"CSV\",\n",
       "    \"metadata\": {},\n",
       "    \"exploration_notebook_arn\": \"arn:aws:sagemaker:us-east-1:082830052325:notebook-instance/ml-factory-exploration-notebook-200136\",\n",
       "    \"transformed_data_s3_location\": \"s3://cn-ml-factory-data-bucket-zt9tmi93zfp0v3q/transformed-data/200136\",\n",
       "    \"aws_notebook_instance_type\": \"ml.t3.medium\",\n",
       "    \"github_url\": \"https://github.com/trilogy-group/cn-ml-factory-Problem-200136\",\n",
       "    \"github_username\": null,\n",
       "    \"data_source\": 91,\n",
       "    \"creator\": 24,\n",
       "    \"creator_email\": \"sanket.agarwal@codenation.co.in\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo Output JSON as table\n",
    "problem = Problem.load(problem_id)\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of tables in this problem\n",
    "tables = problem.table_names()\n",
    "tables=['train_csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to predict\n",
    "# problem.columns(<table_name>)\n",
    "target_column = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the place to explore & visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['covid_data_csv', 'train_csv']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Table names under your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Automatic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and Analize your data automatically based on Pandas Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to the table name you want to explore\n",
    "# Generally, you'd like to see here your main (targe time series) dataset\n",
    "explore_table_name = \"train_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerid</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td></td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passengerid  survived  pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                name     sex   age  sibsp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   parch            ticket     fare cabin embarked  \n",
       "0      0         A/5 21171   7.2500              S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250              S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500              S  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data frame from the exloration table\n",
    "df_explore = problem.read_dataframe_from_table(explore_table_name)\n",
    "df_explore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857edb8e965946da907f287928182f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Summarize dataset'), FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc0ae9e3e6543e794bcf985db6979e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Generate report structure'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Render widgets'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3dd7011ca343539fbc349bbb25eceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# System Cell\n",
    "# Automatic data exploration\n",
    "profile = ProfileReport(df_explore, title=f\"Exploration report for {explore_table_name}\", explorative=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Automatic Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect outliers in your data using MLFactory AnomalyDetection transformation\n",
    "\n",
    "[Read details](https://docs.google.com/document/d/1xyV_paZdy3vW9S954korzOqmZVDX56Yavu2OA0zKT_Y/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        CAUTION: This transformation should be APPLIED AFTER train/test SPLIT on training data to PREVENT DATA LEAKAGE.\n",
      " CAUTION: This transformation requires the DATA TO BE NUMERICAL in nature, i.e. the data passed by the user should have all numerical values (Dtype of all columns should be numeric) otherwise the Transformation will FAIL.\n",
      " Adds is_outlier column where 1 defines an anomaly. User can then manipulate data based on this.\n",
      "    \n",
      "        Required properties: table, name, problem_id\n",
      "    \n",
      "        Attributes: \n",
      "            table    Table. Type = 'string'.  \n",
      "            votes    Number of votes(1-8) to be considered an anomaly. By default 3. Type = 'number'.  \n",
      "            outliers_fraction    Estimated proportion of outliers in the data set (float in (0., 0.5)). By default 0.1. Type = 'number'.  \n",
      "            ignoreHeader    Ignore CSV Header while saving in s3. Type = 'boolean'.  \n",
      "            partitions    Partitions. Type = 'number'. Set number of partitions of output table. \n",
      "    Leave empty if don\"t want to specify partitions. \n",
      "            name    Transformation name. Type = 'string'. The name of the output table. Please ensure that you assign transformation names which are unique in the scope of the current problem. Default = ''anomaly_detection''.\n",
      "            problem_id    Problem Id. Type = 'number'. Id of the problem your are working on. \n",
      "    \n",
      "        Example: \n",
      "            tf.table = \"table1\"\n",
      "            tf.votes = 6\n",
      "            tf.outliers_fraction = 0.2\n",
      "            tf.name = \"my_first_anomaly_detection_transformation\"\n",
      "            tf.problem_id = 147\n",
      "        \n",
      "Transformation saved, id is 936\n",
      "Running AnomalyDetection. Transformation id is 936\n",
      "Execution started. Result will be available in table 'anomaly_detection'. \n",
      "For current execution status, run `tf.tf_execution.refresh_status()`\n",
      "Execution workflow link: https://console.aws.amazon.com/glue/home?region=us-east-1#etl:tab=workflows;workflowView=workflow-run-details?runId=None&workflowName=\n",
      "Logs link: https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/ml-factory/glue-logs;stream=ml_factory_transformation_200136_936_erirlkwc6g\n",
      "\n",
      "Please check your email for 'AWS Notifications' message and confirm subscription, to receive notification when the transformation is done\n",
      "In case you don't see it, most probably your email 'sanket.agarwal@codenation.co.in' is already confirmed for that datasource\n",
      "Please be aware that sometimes, the confirmation email  falls into SPAM folder.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"id\": 936,\n",
       "    \"status\": \"STARTING_TRANSFORMATION\",\n",
       "    \"start_time\": \"2020-12-09 11:53:44.645394\",\n",
       "    \"end_time\": null,\n",
       "    \"glue_workflow_name\": \"\",\n",
       "    \"glue_workflow_run_id\": null,\n",
       "    \"cloudwatch_stream_name\": \"ml_factory_transformation_200136_936_erirlkwc6g\",\n",
       "    \"transformation\": {\n",
       "      \"save\": true,\n",
       "      \"type\": \"anomaly_detection\",\n",
       "      \"name\": \"anomaly_detection_problem_200136\",\n",
       "      \"config\": {\n",
       "        \"name\": \"anomaly_detection\",\n",
       "        \"problem_id\": 200136,\n",
       "        \"table\": \"train_csv\",\n",
       "        \"ignoreHeader\": false\n",
       "      }\n",
       "    },\n",
       "    \"problem\": {\n",
       "      \"id\": 200136,\n",
       "      \"name\": \"Demo\",\n",
       "      \"type\": \"predict\",\n",
       "      \"transformed_data_format\": \"CSV\",\n",
       "      \"metadata\": {},\n",
       "      \"exploration_notebook_arn\": \"arn:aws:sagemaker:us-east-1:082830052325:notebook-instance/ml-factory-exploration-notebook-200136\",\n",
       "      \"transformed_data_s3_location\": \"s3://cn-ml-factory-data-bucket-zt9tmi93zfp0v3q/transformed-data/200136\",\n",
       "      \"aws_notebook_instance_type\": \"ml.t3.medium\",\n",
       "      \"github_url\": \"https://github.com/trilogy-group/cn-ml-factory-Problem-200136\",\n",
       "      \"github_username\": null,\n",
       "      \"data_source\": 91,\n",
       "      \"creator\": 24,\n",
       "      \"creator_email\": \"sanket.agarwal@codenation.co.in\"\n",
       "    },\n",
       "    \"artefact_location\": null\n",
       "  },\n",
       "  \"problem_id\": 200136,\n",
       "  \"transformation_id\": 936\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run this cell if you want to apply automatic anomalies detecion\n",
    "\n",
    "anomaly_detector = Tfs.AnomalyDetection()\n",
    "anomaly_detector.problem_id = problem.id()\n",
    "anomaly_detector.table = explore_table_name\n",
    "anomaly_detector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies detection is still in progress, please wait. Current status is TfExecutionStatus.STARTING_TRANSFORMATION\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell if you appled automatic anomalies detecion above.\n",
    "\n",
    "status = anomaly_detector.tf_execution.refresh_status()\n",
    "if status.is_done():\n",
    "    df_explore = problem.read_dataframe_from_table(anomaly_detector.name)\n",
    "    df_explore.head()\n",
    "else:\n",
    "    print(f\"Anomalies detection is still in progress, please wait. Current status is {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Custom Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to explore more data if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory and Server-Side Transformations Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two options to transform your data:\n",
    " 1. Use `problem.read_dataframe_from_table(<table_name>)` to load all your data in memory into good old pandas DataFrame.\n",
    "  Apply all transformations your need, and save data using `problem.write_dataframe_into_table(<df>, <table_name>)`.\n",
    "  That suits perfectly if you have a small to medium size dataset which fits to RAM (up to several Gbs)\n",
    " 2. Use server-side MLFactory SDK transformations. They run at our backend, take more time (minutes usually),\n",
    "  but can handle, and are intended for, huge datasets.\n",
    "\n",
    "Please find examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations have been divided into 3 sections:\n",
    "1. Transformations before Train Test Split\n",
    "2. Train Test Split\n",
    "3. Transformations after Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server-side transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choose and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. To list all available server-side transformations, invoke \"all()\"\n",
    "\n",
    "# Tfs.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Choose one, e.g. `SelectColumns\n",
    "\n",
    "# tf = Tfs.SelectColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. To get help on a transformation (as well as almost any SDK class or function), invoke `?`,\n",
    "# or put cursor on variable or function name and Shift+Tab\n",
    "\n",
    "# tf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Configure the transformation\n",
    "# Name of the transformation is also used as the name of output table you will find transformed data in\n",
    "# By default, the name is assigned to the name of transformation itself, in snake case\n",
    "\n",
    "\n",
    "# tf.name = \"select_best_columns\"\n",
    "\n",
    "# tf.table = \"raw_data_csv\"\n",
    "\n",
    "# tf.keys = [\"item_id\"]\n",
    "\n",
    "# tf.problem_id = problem.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution = tf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Check the transformation execution status\n",
    "# tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should wait for (usually) several minutes. You can use those flags to understand if execution is complete and if it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution.refresh_status().is_done()\n",
    "# tf_execution.refresh_status().is_successful()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic (Pandas) transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All table names for your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas DataFrame\n",
    "df = problem.read_dataframe_from_table(\"please specify table name\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Change Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do changes using pandas\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Save data back to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check available parameters running the line below\n",
    "# problem.write_dataframe_into_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data\n",
    "# problem.write_dataframe_into_table(df, table_name=<new table name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to Required Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Last Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. If you use server-side transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select transformation class from Tfs.all() list, and replace <SELECT CLASS> with class name\n",
    "tf_final = Tfs.<SELECT CLASS>()\n",
    "# Last transformation in the chain should have the predefined name = \"target_time_series\"\n",
    "# That's required by our backend: we don't specify time series table name explicitly but rather expect data to be\n",
    "# present in a table named \"target_time_series\"\n",
    "tf_final.name = \"target_time_series\"\n",
    "tf_final.partitions = 1\n",
    "tf_final.problem_id = problem.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long running\n",
    "\n",
    "# Run transformation\n",
    "tf_execution = tf_final.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Wait until `is_done` is True\n",
    "tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Or, If you use in-memory pandas transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = problem.read_dataframe_from_table('t1_problem_120')\n",
    "df.head()\n",
    "# Do changes using pandas\n",
    "# ...\n",
    "# Write date\n",
    "# problem.write_dataframe_into_table(df, \"target_time_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations BEFORE Train Test Split\n",
    "\n",
    "Usually Transformations which add features or modify existing features into new ones can be applied before Train Test Split.\n",
    "\n",
    "For example:\n",
    "- Absolute - Computes the absolute value of a number.\n",
    "- SelectColumns - Selects specified columns of interest from the table.\n",
    "\n",
    "Can be applied BEFORE the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations BEFORE splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell to apply train validation test split\n",
    "\n",
    "# train_val_test_splitter = Tfs.Split()\n",
    "# train_val_test_splitter.name = \"\"\n",
    "# train_val_test_splitter.ignoreHeader = True\n",
    "# train_val_test_splitter.table = \"\"\n",
    "# train_val_test_splitter.p = [0.65, 0.15, 0.20]\n",
    "# train_val_test_splitter.problem_id = problem.id()\n",
    "# train_val_test_splitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations AFTER Train Test Split\n",
    "\n",
    "Some Transformations should be applied after the Train Test Split separately as otherwise they can lead to Leakage of Data.\n",
    "\n",
    "These usually include Encoder, Scaler, Normalizer Transformations.\n",
    "\n",
    "For example:\n",
    "- OneHotEncode - Transforms the chosen columns into one-hot representations.\n",
    "- AnomalyDetection - Adds is_outlier column where 1 defines an anomaly\n",
    "\n",
    "Should be applied AFTER the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations AFTER splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "experiment = PredictConfiguration.load(problem_id=problem_id, experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"data\": {\n",
       "      \"experiment\": {\n",
       "        \"id\": 650,\n",
       "        \"transformations\": \"\",\n",
       "        \"name\": \"Demo\",\n",
       "        \"description\": \"demo\",\n",
       "        \"problem_type\": \"Predict\",\n",
       "        \"comparison_metric\": \"rmse\",\n",
       "        \"comparison_metric_optimization_type\": \"Maximize\",\n",
       "        \"custom_fields\": {\n",
       "          \"learning_objective\": \"BinaryClassification\",\n",
       "          \"framework\": \"xgboost\",\n",
       "          \"feature_engineering\": \"AFE\",\n",
       "          \"data_enrichment\": \"no\",\n",
       "          \"explainability\": \"yes\",\n",
       "          \"metric_definitions\": [],\n",
       "          \"positive_data_weight\": 1,\n",
       "          \"sagemaker_experiment_name\": \"Demo-1607510130\"\n",
       "        },\n",
       "        \"dataset\": {},\n",
       "        \"neptune_project_name\": \"cn-ml-factory\",\n",
       "        \"email\": \"cn_ml_factory_service_user@codenation.co.in\",\n",
       "        \"create_time\": \"2020-12-09T10:35:31.024104Z\",\n",
       "        \"last_updated_time\": \"2020-12-09T10:35:32.216530Z\",\n",
       "        \"status\": \"Running Trials\",\n",
       "        \"reason\": \"\",\n",
       "        \"notebook_link\": \"https://ml-factory-exploration-notebook-200136.notebook.us-east-1.sagemaker.aws/notebooks/cn-ml-factory-Problem-200136/ml-factory-exploration-notebook-200136/SageMaker/Demo.ipynb\",\n",
       "        \"problem\": 200136,\n",
       "        \"neptune_project_link\": \"https://ui.neptune.ai/cn-ml-factory-user/cn-ml-factory\",\n",
       "        \"xray_link\": \"https://console.aws.amazon.com/xray/home?region=us-east-1#/traces?timeRange=PT5M\",\n",
       "        \"best_trial_deployment_data\": null,\n",
       "        \"best_trial_triggered_model_data\": null,\n",
       "        \"best_trial_metrics\": null,\n",
       "        \"problem_id\": 200136\n",
       "      }\n",
       "    }\n",
       "  },\n",
       "  \"name\": \"Demo\",\n",
       "  \"description\": \"demo\",\n",
       "  \"neptune_project_name\": \"cn-ml-factory\",\n",
       "  \"training_data_s3_uri\": null,\n",
       "  \"validation_data_s3_uri\": null,\n",
       "  \"test_data_s3_uri\": null,\n",
       "  \"learning_objective\": null,\n",
       "  \"comparison_metric\": null,\n",
       "  \"custom_fields\": null,\n",
       "  \"positive_data_weight\": 1.0,\n",
       "  \"metric_definitions\": [],\n",
       "  \"hpt_max_jobs\": 5,\n",
       "  \"hpt_max_parallel_jobs\": 5,\n",
       "  \"train_instance_count\": 1,\n",
       "  \"train_instance_type\": \"ml.m4.xlarge\",\n",
       "  \"test_instance_count\": 1,\n",
       "  \"test_instance_type\": \"ml.m4.xlarge\",\n",
       "  \"data_enrichers\": null\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Fields (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those fields are mandatory, and require you to set values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to [the XGBoost Documentation](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters) for all possible `learning_objective`s and `comparison_metric`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please give your experiment a meaninful name\n",
    "# It should be unique in the scope of the problem\n",
    "experiment.name = \"Demo\"\n",
    "experiment.description = \"demo\"\n",
    "\n",
    "#specify the dataset for the experiment\n",
    "# Format is : <train_test_split_transformation_name>_problem_<problem_id>_<i>\n",
    "# i=0 for train data, i=1 for valid data and i=2 for test data\n",
    "\n",
    "experiment.training_data_s3_uri = \"\"\n",
    "experiment.validation_data_s3_uri = \"\"\n",
    "experiment.test_data_s3_uri = \"\"\n",
    "\n",
    "# Valid Pairs include\n",
    "# binary:logistic -> error  | auc | aucpr\n",
    "# multi:softprob  -> merror | auc | aucpr\n",
    "# reg:logistic    -> rmse  | rmsle | mae | mape\n",
    "experiment.learning_objective = \"BinaryClassification\"\n",
    "experiment.comparison_metric = \"rmse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can leave the values as is, or adjust them if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment and change any of these default values\n",
    "\n",
    "# experiment.positive_data_weight: float = 1.0\n",
    "# experiment.hpt_max_jobs: int = 5\n",
    "# experiment.hpt_max_parallel_jobs: int = 5\n",
    "# experiment.train_instance_count: int = 1\n",
    "# experiment.train_instance_type: str = \"ml.m4.xlarge\"\n",
    "# experiment.test_instance_count: int = 1\n",
    "# experiment.test_instance_type: str = \"ml.m4.xlarge\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running training make sure\n",
    "  - first column contains the target value  [use : MoveColumnToFirstGlueTransformer]\n",
    "  - the dataset headers are extracted and uploaded to S3 [use : UpdateColumnsGlueTransformer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "experiment.save_and_run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now all you need is to wait until training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-df3f541e0f10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Training could take several hours, or even more, on huge datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperimentLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/mlfactory/sdk/experiment/base.py\u001b[0m in \u001b[0;36mstatus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mintercept_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# When the status = 'Training complete', we can proceed further. Otherwise, we need to wait.\n",
    "# Training could take several hours, or even more, on huge datasets\n",
    "experiment = ExperimentLoader.load(problem_id, experiment_id)\n",
    "experiment.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find the model valuable, you might consider deploying it to reuse in the future.\n",
    "Uncomment and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "\n",
    "# experiment.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# When deployment is done, you can run inferences using the inference link\n",
    "# experiment.inference_link()\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda_amazonei_tensorflow_p36"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
       "// on document structure change\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
    "// on document structure change\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764\n"
     ]
    }
   ],
   "source": [
    "# Parameter: URL of the REST API\n",
    "%env MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Problem is created in UI, here we use its ID as parameter\n",
    "problem_id = 200136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Experiment is pre-created in UI, here we use its ID as parameter to load and reuse it\n",
    "experiment_id = 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: No metadata found in /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages\u001b[0m\r\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/s3transfer-0.3.3.dist-info/METADATA'\r\n",
      "\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install MLFactory SDK\n",
    "!pip install mlfactory_sdk --upgrade --extra-index https://pypi.swarm.devfactory.com > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    argon2-cffi-20.1.0         |   py36h7b6447c_1          46 KB\n",
      "    astroid-2.4.2              |           py36_0         279 KB\n",
      "    async_generator-1.10       |   py36h28b3542_0          39 KB\n",
      "    docutils-0.16              |           py36_1         669 KB\n",
      "    jupyterlab-2.2.6           |             py_0         3.4 MB\n",
      "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
      "    jupyterlab_server-1.2.0    |             py_0          25 KB\n",
      "    keras-2.4.3                |                0          12 KB\n",
      "    keras-base-2.4.3           |             py_0          37 KB\n",
      "    nb_conda_kernels-2.3.1     |   py36h06a4308_0          28 KB\n",
      "    nbclient-0.5.1             |             py_0          58 KB\n",
      "    nbconvert-6.0.7            |           py36_0         480 KB\n",
      "    nest-asyncio-1.4.3         |     pyhd3eb1b0_0          11 KB\n",
      "    notebook-6.1.4             |           py36_0         4.1 MB\n",
      "    numpydoc-1.1.0             |     pyhd3eb1b0_1          42 KB\n",
      "    pylint-2.6.0               |           py36_0         443 KB\n",
      "    python-language-server-0.36.1|     pyhd3eb1b0_1          41 KB\n",
      "    sphinx-3.2.1               |             py_0         1.1 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        10.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  argon2-cffi        pkgs/main/linux-64::argon2-cffi-20.1.0-py36h7b6447c_1\n",
      "  async_generator    pkgs/main/linux-64::async_generator-1.10-py36h28b3542_0\n",
      "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py36h27cfd23_1003\n",
      "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0\n",
      "  keras-base         pkgs/main/noarch::keras-base-2.4.3-py_0\n",
      "  nbclient           pkgs/main/noarch::nbclient-0.5.1-py_0\n",
      "  nest-asyncio       pkgs/main/noarch::nest-asyncio-1.4.3-pyhd3eb1b0_0\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  binutils_impl_linux-64-2.33.1-he6710b0_7\n",
      "  binutils_linux-64-2.33.1-h9595d00_15\n",
      "  gcc_impl_linux-64-7.3.0-habb00fd_1\n",
      "  gcc_linux-64-7.3.0-h553295d_15\n",
      "  gxx_impl_linux-64-7.3.0-hdf63c60_1\n",
      "  gxx_linux-64-7.3.0-h553295d_15\n",
      "  keras-applications-1.0.8-py_1\n",
      "  libgpuarray-0.7.6-h14c3975_0\n",
      "  mako-1.1.3-py_0\n",
      "  pygpu-0.7.6-py36heb32a55_0\n",
      "  theano-1.0.4-py36hfd86e86_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  astroid                                      2.3.3-py36_0 --> 2.4.2-py36_0\n",
      "  bleach             pkgs/main/linux-64::bleach-3.1.0-py36~ --> pkgs/main/noarch::bleach-3.2.1-py_0\n",
      "  docutils                                      0.16-py36_0 --> 0.16-py36_1\n",
      "  isort              pkgs/main/linux-64::isort-4.3.21-py36~ --> pkgs/main/noarch::isort-5.6.4-py_0\n",
      "  jupyterlab                             1.2.6-pyhf63ae98_0 --> 2.2.6-py_0\n",
      "  jupyterlab_server                              1.0.6-py_0 --> 1.2.0-py_0\n",
      "  keras              conda-forge/linux-64::keras-2.3.1-py3~ --> pkgs/main/noarch::keras-2.4.3-0\n",
      "  nb_conda_kernels                             2.2.4-py36_0 --> 2.3.1-py36h06a4308_0\n",
      "  nbconvert                                    5.6.1-py36_0 --> 6.0.7-py36_0\n",
      "  notebook                                     6.0.3-py36_0 --> 6.1.4-py36_0\n",
      "  numpydoc                                       0.9.2-py_0 --> 1.1.0-pyhd3eb1b0_1\n",
      "  pip                                         20.0.2-py36_1 --> 20.3.1-py36h06a4308_0\n",
      "  pylint                                       2.4.4-py36_0 --> 2.6.0-py36_0\n",
      "  python-language-s~                            0.35.1-py_0 --> 0.36.1-pyhd3eb1b0_1\n",
      "  sphinx                                         2.4.0-py_0 --> 3.2.1-py_0\n",
      "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py~ --> pkgs/main/noarch::urllib3-1.25.11-py_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  lazy-object-proxy                    1.5.2-py36h27cfd23_0 --> 1.4.3-py36h7b6447c_0\n",
      "  wrapt                               1.12.1-py36h7b6447c_1 --> 1.11.2-py36h7b6447c_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "sphinx-3.2.1         | 1.1 MB    | ##################################### | 100% \n",
      "pylint-2.6.0         | 443 KB    | ##################################### | 100% \n",
      "nbconvert-6.0.7      | 480 KB    | ##################################### | 100% \n",
      "keras-base-2.4.3     | 37 KB     | ##################################### | 100% \n",
      "nest-asyncio-1.4.3   | 11 KB     | ##################################### | 100% \n",
      "jupyterlab_server-1. | 25 KB     | ##################################### | 100% \n",
      "numpydoc-1.1.0       | 42 KB     | ##################################### | 100% \n",
      "nb_conda_kernels-2.3 | 28 KB     | ##################################### | 100% \n",
      "docutils-0.16        | 669 KB    | ##################################### | 100% \n",
      "async_generator-1.10 | 39 KB     | ##################################### | 100% \n",
      "argon2-cffi-20.1.0   | 46 KB     | ##################################### | 100% \n",
      "python-language-serv | 41 KB     | ##################################### | 100% \n",
      "notebook-6.1.4       | 4.1 MB    | ##################################### | 100% \n",
      "keras-2.4.3          | 12 KB     | ##################################### | 100% \n",
      "jupyterlab_pygments- | 8 KB      | ##################################### | 100% \n",
      "jupyterlab-2.2.6     | 3.4 MB    | ##################################### | 100% \n",
      "astroid-2.4.2        | 279 KB    | ##################################### | 100% \n",
      "nbclient-0.5.1       | 58 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: - b'Disabling nb_conda_kernels...\\nStatus: disabled\\n'\n",
      "| b'Enabling nb_conda_kernels...\\nCONDA_PREFIX: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\\nStatus: enabled\\n'\n",
      "done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36\n",
      "\n",
      "  added / updated specs:\n",
      "    - imagehash\n",
      "    - pandas-profiling\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.12.5  |       ha878542_0         137 KB  conda-forge\n",
      "    certifi-2020.12.5          |   py36h5fab9bb_0         143 KB  conda-forge\n",
      "    confuse-1.4.0              |     pyhd3deb0d_0          21 KB  conda-forge\n",
      "    htmlmin-0.1.12             |             py_1          21 KB  conda-forge\n",
      "    imagehash-4.2.0            |     pyhd8ed1ab_0         292 KB  conda-forge\n",
      "    missingno-0.4.2            |             py_1          12 KB  conda-forge\n",
      "    pandas-profiling-2.9.0     |     pyh9f0ad1d_0         176 KB  conda-forge\n",
      "    phik-0.10.0                |             py_0         578 KB  conda-forge\n",
      "    seaborn-base-0.11.0        |     pyhd8ed1ab_1         216 KB  conda-forge\n",
      "    tangled-up-in-unicode-0.0.6|     pyh9f0ad1d_0         2.0 MB  conda-forge\n",
      "    visions-0.5.0              |     pyh9f0ad1d_0          42 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  confuse            conda-forge/noarch::confuse-1.4.0-pyhd3deb0d_0\n",
      "  htmlmin            conda-forge/noarch::htmlmin-0.1.12-py_1\n",
      "  imagehash          conda-forge/noarch::imagehash-4.2.0-pyhd8ed1ab_0\n",
      "  missingno          conda-forge/noarch::missingno-0.4.2-py_1\n",
      "  pandas-profiling   conda-forge/noarch::pandas-profiling-2.9.0-pyh9f0ad1d_0\n",
      "  phik               conda-forge/noarch::phik-0.10.0-py_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "  seaborn-base       conda-forge/noarch::seaborn-base-0.11.0-pyhd8ed1ab_1\n",
      "  tangled-up-in-uni~ conda-forge/noarch::tangled-up-in-unicode-0.0.6-pyh9f0ad1d_0\n",
      "  visions            conda-forge/noarch::visions-0.5.0-pyh9f0ad1d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2020.10.14~ --> conda-forge::ca-certificates-2020.12.5-ha878542_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2020.12.5-py36h06a~ --> conda-forge::certifi-2020.12.5-py36h5fab9bb_0\n",
      "  openssl              pkgs/main::openssl-1.1.1h-h7b6447c_0 --> conda-forge::openssl-1.1.1h-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visions-0.5.0        | 42 KB     | ##################################### | 100% \n",
      "missingno-0.4.2      | 12 KB     | ##################################### | 100% \n",
      "imagehash-4.2.0      | 292 KB    | ##################################### | 100% \n",
      "confuse-1.4.0        | 21 KB     | ##################################### | 100% \n",
      "pandas-profiling-2.9 | 176 KB    | ##################################### | 100% \n",
      "htmlmin-0.1.12       | 21 KB     | ##################################### | 100% \n",
      "phik-0.10.0          | 578 KB    | ##################################### | 100% \n",
      "tangled-up-in-unicod | 2.0 MB    | ##################################### | 100% \n",
      "certifi-2020.12.5    | 143 KB    | ##################################### | 100% \n",
      "ca-certificates-2020 | 137 KB    | ##################################### | 100% \n",
      "seaborn-base-0.11.0  | 216 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: / "
     ]
    }
   ],
   "source": [
    "# WARNING: This cell may take 15-20 mins to finish. If you don'y need Pandas Profiler data analysis, just skip this cell,\n",
    "# as well as `a. Automatic Data Exploration` section\n",
    "\n",
    "# System cell\n",
    "# Sophisticated way to import pandas profiler into the Notebook. Work with kernel = 'conda_amazonei_tensorflow_p36\n",
    "try:\n",
    "    import pandas_profiling\n",
    "except:\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 --all -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda install -c conda-forge -n amazonei_tensorflow_p36 pandas-profiling imagehash -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 ipywidgets -y\n",
    "finally:\n",
    "    import pandas_profiling\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import required and common libs\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pyarrow\n",
    "from itables import show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c defaults -c conda-forge shap # > /dev/null\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import all required MLFactory classes, and init MLFactory API\n",
    "from mlfactory.sdk.restclient.mlfactory_api import MLFactoryApi\n",
    "from mlfactory.sdk.common import Util\n",
    "from mlfactory.sdk.problem.base import Problem\n",
    "from mlfactory.sdk.tf.transformations import Tfs\n",
    "from mlfactory.sdk.common import ExperimentLoader\n",
    "from mlfactory.sdk.tf.tf_execution import TfExecution\n",
    "\n",
    "\n",
    "mlf_api = MLFactoryApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict specific imports\n",
    "import hiplot as hip\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.pyplot import xticks\n",
    "%matplotlib inline\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "from mlfactory.sdk.predict.predict_config import PredictConfiguration\n",
    "from mlfactory.sdk.aws_autopilot.aws_autopilot_config import AWSAutopilotConfiguration\n",
    "from mlfactory.sdk.afe.afe_config import AFEConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_config import ExplainabilityConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_controller import ExplainabilityController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"entity_json\": {\n",
       "    \"id\": 200136,\n",
       "    \"name\": \"Demo\",\n",
       "    \"type\": \"predict\",\n",
       "    \"transformed_data_format\": \"CSV\",\n",
       "    \"metadata\": {},\n",
       "    \"exploration_notebook_arn\": \"arn:aws:sagemaker:us-east-1:082830052325:notebook-instance/ml-factory-exploration-notebook-200136\",\n",
       "    \"transformed_data_s3_location\": \"s3://cn-ml-factory-data-bucket-zt9tmi93zfp0v3q/transformed-data/200136\",\n",
       "    \"aws_notebook_instance_type\": \"ml.t3.medium\",\n",
       "    \"github_url\": \"https://github.com/trilogy-group/cn-ml-factory-Problem-200136\",\n",
       "    \"github_username\": null,\n",
       "    \"data_source\": 91,\n",
       "    \"creator\": 24,\n",
       "    \"creator_email\": \"sanket.agarwal@codenation.co.in\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo Output JSON as table\n",
    "problem = Problem.load(problem_id)\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of tables in this problem\n",
    "tables = problem.table_names()\n",
    "tables=['train_csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to predict\n",
    "# problem.columns(<table_name>)\n",
    "target_column = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the place to explore & visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table names under your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Automatic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and Analize your data automatically based on Pandas Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to the table name you want to explore\n",
    "# Generally, you'd like to see here your main (targe time series) dataset\n",
    "explore_table_name = \"raw_data_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame from the exloration table\n",
    "df_explore = problem.read_dataframe_from_table(explore_table_name)\n",
    "df_explore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Cell\n",
    "# Automatic data exploration\n",
    "profile = ProfileReport(df_explore, title=f\"Exploration report for {explore_table_name}\", explorative=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Automatic Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect outliers in your data using MLFactory AnomalyDetection transformation\n",
    "\n",
    "[Read details](https://docs.google.com/document/d/1xyV_paZdy3vW9S954korzOqmZVDX56Yavu2OA0zKT_Y/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you want to apply automatic anomalies detecion\n",
    "\n",
    "anomaly_detector = Tfs.AnomalyDetection()\n",
    "anomaly_detector.problem_id = problem.id()\n",
    "anomaly_detector.table = explore_table_name\n",
    "anomaly_detector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you appled automatic anomalies detecion above.\n",
    "\n",
    "status = anomaly_detector.tf_execution.refresh_status()\n",
    "if status.is_done():\n",
    "    df_explore = problem.read_dataframe_from_table(anomaly_detector.name)\n",
    "    df_explore.head()\n",
    "else:\n",
    "    print(f\"Anomalies detection is still in progress, please wait. Current status is {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Custom Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to explore more data if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory and Server-Side Transformations Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two options to transform your data:\n",
    " 1. Use `problem.read_dataframe_from_table(<table_name>)` to load all your data in memory into good old pandas DataFrame.\n",
    "  Apply all transformations your need, and save data using `problem.write_dataframe_into_table(<df>, <table_name>)`.\n",
    "  That suits perfectly if you have a small to medium size dataset which fits to RAM (up to several Gbs)\n",
    " 2. Use server-side MLFactory SDK transformations. They run at our backend, take more time (minutes usually),\n",
    "  but can handle, and are intended for, huge datasets.\n",
    "\n",
    "Please find examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations have been divided into 3 sections:\n",
    "1. Transformations before Train Test Split\n",
    "2. Train Test Split\n",
    "3. Transformations after Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server-side transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choose and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. To list all available server-side transformations, invoke \"all()\"\n",
    "\n",
    "# Tfs.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Choose one, e.g. `SelectColumns\n",
    "\n",
    "# tf = Tfs.SelectColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. To get help on a transformation (as well as almost any SDK class or function), invoke `?`,\n",
    "# or put cursor on variable or function name and Shift+Tab\n",
    "\n",
    "# tf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Configure the transformation\n",
    "# Name of the transformation is also used as the name of output table you will find transformed data in\n",
    "# By default, the name is assigned to the name of transformation itself, in snake case\n",
    "\n",
    "\n",
    "# tf.name = \"select_best_columns\"\n",
    "\n",
    "# tf.table = \"raw_data_csv\"\n",
    "\n",
    "# tf.keys = [\"item_id\"]\n",
    "\n",
    "# tf.problem_id = problem.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution = tf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Check the transformation execution status\n",
    "# tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should wait for (usually) several minutes. You can use those flags to understand if execution is complete and if it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution.refresh_status().is_done()\n",
    "# tf_execution.refresh_status().is_successful()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic (Pandas) transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All table names for your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas DataFrame\n",
    "df = problem.read_dataframe_from_table(\"please specify table name\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Change Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do changes using pandas\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Save data back to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check available parameters running the line below\n",
    "# problem.write_dataframe_into_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data\n",
    "# problem.write_dataframe_into_table(df, table_name=<new table name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to Required Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Last Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. If you use server-side transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select transformation class from Tfs.all() list, and replace <SELECT CLASS> with class name\n",
    "tf_final = Tfs.<SELECT CLASS>()\n",
    "# Last transformation in the chain should have the predefined name = \"target_time_series\"\n",
    "# That's required by our backend: we don't specify time series table name explicitly but rather expect data to be\n",
    "# present in a table named \"target_time_series\"\n",
    "tf_final.name = \"target_time_series\"\n",
    "tf_final.partitions = 1\n",
    "tf_final.problem_id = problem.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long running\n",
    "\n",
    "# Run transformation\n",
    "tf_execution = tf_final.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Wait until `is_done` is True\n",
    "tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Or, If you use in-memory pandas transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = problem.read_dataframe_from_table('t1_problem_120')\n",
    "df.head()\n",
    "# Do changes using pandas\n",
    "# ...\n",
    "# Write date\n",
    "# problem.write_dataframe_into_table(df, \"target_time_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations BEFORE Train Test Split\n",
    "\n",
    "Usually Transformations which add features or modify existing features into new ones can be applied before Train Test Split.\n",
    "\n",
    "For example:\n",
    "- Absolute - Computes the absolute value of a number.\n",
    "- SelectColumns - Selects specified columns of interest from the table.\n",
    "\n",
    "Can be applied BEFORE the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations BEFORE splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell to apply train validation test split\n",
    "\n",
    "# train_val_test_splitter = Tfs.Split()\n",
    "# train_val_test_splitter.name = \"\"\n",
    "# train_val_test_splitter.ignoreHeader = True\n",
    "# train_val_test_splitter.table = \"\"\n",
    "# train_val_test_splitter.p = [0.65, 0.15, 0.20]\n",
    "# train_val_test_splitter.problem_id = problem.id()\n",
    "# train_val_test_splitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations AFTER Train Test Split\n",
    "\n",
    "Some Transformations should be applied after the Train Test Split separately as otherwise they can lead to Leakage of Data.\n",
    "\n",
    "These usually include Encoder, Scaler, Normalizer Transformations.\n",
    "\n",
    "For example:\n",
    "- OneHotEncode - Transforms the chosen columns into one-hot representations.\n",
    "- AnomalyDetection - Adds is_outlier column where 1 defines an anomaly\n",
    "\n",
    "Should be applied AFTER the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations AFTER splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "experiment = PredictConfiguration.load(problem_id=problem_id, experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Fields (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those fields are mandatory, and require you to set values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to [the XGBoost Documentation](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters) for all possible `learning_objective`s and `comparison_metric`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please give your experiment a meaninful name\n",
    "# It should be unique in the scope of the problem\n",
    "experiment.name = \"Demo\"\n",
    "experiment.description = \"demo\"\n",
    "\n",
    "#specify the dataset for the experiment\n",
    "# Format is : <train_test_split_transformation_name>_problem_<problem_id>_<i>\n",
    "# i=0 for train data, i=1 for valid data and i=2 for test data\n",
    "\n",
    "experiment.training_data_s3_uri = \"\"\n",
    "experiment.validation_data_s3_uri = \"\"\n",
    "experiment.test_data_s3_uri = \"\"\n",
    "\n",
    "# Valid Pairs include\n",
    "# binary:logistic -> error  | auc | aucpr\n",
    "# multi:softprob  -> merror | auc | aucpr\n",
    "# reg:logistic    -> rmse  | rmsle | mae | mape\n",
    "experiment.learning_objective = \"BinaryClassification\"\n",
    "experiment.comparison_metric = \"rmse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can leave the values as is, or adjust them if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment and change any of these default values\n",
    "\n",
    "# experiment.positive_data_weight: float = 1.0\n",
    "# experiment.hpt_max_jobs: int = 5\n",
    "# experiment.hpt_max_parallel_jobs: int = 5\n",
    "# experiment.train_instance_count: int = 1\n",
    "# experiment.train_instance_type: str = \"ml.m4.xlarge\"\n",
    "# experiment.test_instance_count: int = 1\n",
    "# experiment.test_instance_type: str = \"ml.m4.xlarge\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running training make sure\n",
    "  - first column contains the target value  [use : MoveColumnToFirstGlueTransformer]\n",
    "  - the dataset headers are extracted and uploaded to S3 [use : UpdateColumnsGlueTransformer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "experiment.save_and_run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now all you need is to wait until training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the status = 'Training complete', we can proceed further. Otherwise, we need to wait.\n",
    "# Training could take several hours, or even more, on huge datasets\n",
    "experiment = ExperimentLoader.load(problem_id, experiment_id)\n",
    "experiment.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find the model valuable, you might consider deploying it to reuse in the future.\n",
    "Uncomment and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "\n",
    "# experiment.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# When deployment is done, you can run inferences using the inference link\n",
    "# experiment.inference_link()\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda_amazonei_tensorflow_p36"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
       "// on document structure change\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Javascript to generate Table of Contents from notebook headers. Re-execute it at the very begining and\n",
    "// on document structure change\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764\n"
     ]
    }
   ],
   "source": [
    "# Parameter: URL of the REST API\n",
    "%env MLFACTORY_REST_API_URL=http://dl1.aureacentral.com:11764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Problem is created in UI, here we use its ID as parameter\n",
    "problem_id = 200136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter\n",
    "# Experiment is pre-created in UI, here we use its ID as parameter to load and reuse it\n",
    "experiment_id = 650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_tensorflow_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install MLFactory SDK\n",
    "!pip install mlfactory_sdk --upgrade --extra-index https://pypi.swarm.devfactory.com > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/linux-64::pandas==1.0.1=py36h0573a6f_0\n",
      "  - conda-forge/linux-64::keras==2.3.1=py36_0\n",
      "  - defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0\n",
      "  - defaults/linux-64::scikit-learn==0.22.1=py36hd81dba3_0\n",
      "  - defaults/linux-64::python-language-server==0.31.7=py36_0\n",
      "  - defaults/linux-64::bkcharts==0.2=py36_0\n",
      "  - defaults/linux-64::nb_conda==2.2.1=py36_0\n",
      "  - defaults/noarch::numpydoc==0.9.2=py_0\n",
      "  - defaults/linux-64::pytest-arraydiff==0.3=py36h39e3cac_0\n",
      "  - defaults/linux-64::bottleneck==1.3.2=py36heb32a55_0\n",
      "  - defaults/noarch::sphinx==2.4.0=py_0\n",
      "  - defaults/linux-64::pywavelets==1.1.1=py36h7b6447c_0\n",
      "  - defaults/noarch::pytest-astropy==0.8.0=py_0\n",
      "  - defaults/linux-64::numexpr==2.7.1=py36h423224d_0\n",
      "  - defaults/noarch::anaconda-project==0.8.4=py_0\n",
      "  - conda-forge/linux-64::pygpu==0.7.6=py36h68bb277_1002\n",
      "  - defaults/linux-64::nbconvert==5.6.1=py36_0\n",
      "  - defaults/linux-64::h5py==2.10.0=py36h7918eee_0\n",
      "  - defaults/linux-64::bokeh==1.4.0=py36_0\n",
      "  - defaults/noarch::jupyterlab_server==1.0.6=py_0\n",
      "  - conda-forge/noarch::keras-preprocessing==1.1.0=py_0\n",
      "  - defaults/linux-64::numpy-base==1.18.1=py36hde5b4d6_1\n",
      "  - defaults/linux-64::jupyter==1.0.0=py36_7\n",
      "  - defaults/linux-64::astropy==4.0=py36h7b6447c_0\n",
      "  - defaults/linux-64::patsy==0.5.1=py36_0\n",
      "  - defaults/linux-64::scikit-image==0.16.2=py36h0573a6f_0\n",
      "  - defaults/linux-64::matplotlib-base==3.1.3=py36hef1b27d_0\n",
      "  - defaults/linux-64::imageio==2.6.1=py36_0\n",
      "  - defaults/linux-64::pytables==3.6.1=py36h71ec239_0\n",
      "  - conda-forge/noarch::keras-applications==1.0.8=py_1\n",
      "  - defaults/linux-64::nb_conda_kernels==2.2.4=py36_0\n",
      "  - defaults/linux-64::mkl_fft==1.0.15=py36ha843d7b_0\n",
      "  - defaults/linux-64::statsmodels==0.11.0=py36h7b6447c_0\n",
      "  - defaults/linux-64::spyder==4.0.1=py36_0\n",
      "  - defaults/noarch::seaborn==0.10.0=py_0\n",
      "  - defaults/linux-64::requests==2.22.0=py36_1\n",
      "  - defaults/linux-64::numba==0.48.0=py36h0573a6f_0\n",
      "  - defaults/linux-64::scipy==1.4.1=py36h0b6359f_0\n",
      "  - defaults/noarch::pytest-doctestplus==0.5.0=py_0\n",
      "  - defaults/linux-64::mkl_random==1.1.0=py36hd6b4f25_0\n",
      "  - defaults/noarch::dask==2.11.0=py_0\n",
      "  - conda-forge/linux-64::theano==1.0.3=py36hfc679d8_1\n",
      "  - defaults/noarch::ipywidgets==7.5.1=py_0\n",
      "  - defaults/linux-64::widgetsnbextension==3.5.1=py36_0\n",
      "  - defaults/noarch::s3fs==0.4.2=py_0\n",
      "  - defaults/linux-64::notebook==6.0.3=py36_0\n",
      "  - defaults/linux-64::matplotlib==3.1.3=py36_0\n",
      "  - defaults/linux-64::anaconda-client==1.7.2=py36_0\n",
      "  - defaults/linux-64::numpy==1.18.1=py36h4f9e942_0\n",
      "failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: / \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/linux-64::keras==2.3.1=py36_0\n",
      "  - defaults/noarch::jupyterlab==1.2.6=pyhf63ae98_0\n",
      "  - defaults/linux-64::python-language-server==0.31.7=py36_0\n",
      "  - defaults/linux-64::nb_conda==2.2.1=py36_0\n",
      "  - defaults/noarch::numpydoc==0.9.2=py_0\n",
      "  - defaults/noarch::sphinx==2.4.0=py_0\n",
      "  - defaults/noarch::anaconda-project==0.8.4=py_0\n",
      "  - defaults/linux-64::nbconvert==5.6.1=py36_0\n",
      "  - defaults/noarch::jupyterlab_server==1.0.6=py_0\n",
      "  - defaults/linux-64::jupyter==1.0.0=py36_7\n",
      "  - defaults/linux-64::nb_conda_kernels==2.2.4=py36_0\n",
      "  - defaults/linux-64::spyder==4.0.1=py36_0\n",
      "  - defaults/linux-64::requests==2.22.0=py36_1\n",
      "  - defaults/noarch::ipywidgets==7.5.1=py_0\n",
      "  - defaults/linux-64::widgetsnbextension==3.5.1=py36_0\n",
      "  - defaults/noarch::s3fs==0.4.2=py_0\n",
      "  - defaults/linux-64::notebook==6.0.3=py36_0\n",
      "  - defaults/linux-64::anaconda-client==1.7.2=py36_0\n",
      "- "
     ]
    }
   ],
   "source": [
    "# WARNING: This cell may take 15-20 mins to finish. If you don'y need Pandas Profiler data analysis, just skip this cell,\n",
    "# as well as `a. Automatic Data Exploration` section\n",
    "\n",
    "# System cell\n",
    "# Sophisticated way to import pandas profiler into the Notebook. Work with kernel = 'conda_amazonei_tensorflow_p36\n",
    "try:\n",
    "    import pandas_profiling\n",
    "except:\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 --all -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda install -c conda-forge -n amazonei_tensorflow_p36 pandas-profiling imagehash -y\n",
    "    !sudo /home/ec2-user/anaconda3/bin/conda update -n amazonei_tensorflow_p36 ipywidgets -y\n",
    "finally:\n",
    "    import pandas_profiling\n",
    "\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import required and common libs\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pyarrow\n",
    "from itables import show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y -c defaults -c conda-forge shap # > /dev/null\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Import all required MLFactory classes, and init MLFactory API\n",
    "from mlfactory.sdk.restclient.mlfactory_api import MLFactoryApi\n",
    "from mlfactory.sdk.common import Util\n",
    "from mlfactory.sdk.problem.base import Problem\n",
    "from mlfactory.sdk.tf.transformations import Tfs\n",
    "from mlfactory.sdk.common import ExperimentLoader\n",
    "from mlfactory.sdk.tf.tf_execution import TfExecution\n",
    "\n",
    "\n",
    "mlf_api = MLFactoryApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict specific imports\n",
    "import hiplot as hip\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib.pyplot import xticks\n",
    "%matplotlib inline\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "from mlfactory.sdk.predict.predict_config import PredictConfiguration\n",
    "from mlfactory.sdk.aws_autopilot.aws_autopilot_config import AWSAutopilotConfiguration\n",
    "from mlfactory.sdk.afe.afe_config import AFEConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_config import ExplainabilityConfiguration\n",
    "from mlfactory.sdk.explainability.explainability_controller import ExplainabilityController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo Output JSON as table\n",
    "problem = Problem.load(problem_id)\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of tables in this problem\n",
    "tables = problem.table_names()\n",
    "tables=['train_csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column to predict\n",
    "# problem.columns(<table_name>)\n",
    "target_column = 'Survived'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the place to explore & visualize your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table names under your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Automatic Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and Analize your data automatically based on Pandas Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to the table name you want to explore\n",
    "# Generally, you'd like to see here your main (targe time series) dataset\n",
    "explore_table_name = \"train_csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame from the exloration table\n",
    "df_explore = problem.read_dataframe_from_table(explore_table_name)\n",
    "df_explore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Cell\n",
    "# Automatic data exploration\n",
    "profile = ProfileReport(df_explore, title=f\"Exploration report for {explore_table_name}\", explorative=True)\n",
    "profile.to_widgets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Automatic Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect outliers in your data using MLFactory AnomalyDetection transformation\n",
    "\n",
    "[Read details](https://docs.google.com/document/d/1xyV_paZdy3vW9S954korzOqmZVDX56Yavu2OA0zKT_Y/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you want to apply automatic anomalies detecion\n",
    "\n",
    "anomaly_detector = Tfs.AnomalyDetection()\n",
    "anomaly_detector.problem_id = problem.id()\n",
    "anomaly_detector.table = explore_table_name\n",
    "anomaly_detector.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you appled automatic anomalies detecion above.\n",
    "\n",
    "status = anomaly_detector.tf_execution.refresh_status()\n",
    "if status.is_done():\n",
    "    df_explore = problem.read_dataframe_from_table(anomaly_detector.name)\n",
    "    df_explore.head()\n",
    "else:\n",
    "    print(f\"Anomalies detection is still in progress, please wait. Current status is {status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Custom Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to explore more data if you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory and Server-Side Transformations Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two options to transform your data:\n",
    " 1. Use `problem.read_dataframe_from_table(<table_name>)` to load all your data in memory into good old pandas DataFrame.\n",
    "  Apply all transformations your need, and save data using `problem.write_dataframe_into_table(<df>, <table_name>)`.\n",
    "  That suits perfectly if you have a small to medium size dataset which fits to RAM (up to several Gbs)\n",
    " 2. Use server-side MLFactory SDK transformations. They run at our backend, take more time (minutes usually),\n",
    "  but can handle, and are intended for, huge datasets.\n",
    "\n",
    "Please find examples below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations have been divided into 3 sections:\n",
    "1. Transformations before Train Test Split\n",
    "2. Train Test Split\n",
    "3. Transformations after Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server-side transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Choose and configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. To list all available server-side transformations, invoke \"all()\"\n",
    "\n",
    "# Tfs.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Choose one, e.g. `SelectColumns\n",
    "\n",
    "# tf = Tfs.SelectColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. To get help on a transformation (as well as almost any SDK class or function), invoke `?`,\n",
    "# or put cursor on variable or function name and Shift+Tab\n",
    "\n",
    "# tf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Configure the transformation\n",
    "# Name of the transformation is also used as the name of output table you will find transformed data in\n",
    "# By default, the name is assigned to the name of transformation itself, in snake case\n",
    "\n",
    "\n",
    "# tf.name = \"select_best_columns\"\n",
    "\n",
    "# tf.table = \"raw_data_csv\"\n",
    "\n",
    "# tf.keys = [\"item_id\"]\n",
    "\n",
    "# tf.problem_id = problem.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution = tf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Check the transformation execution status\n",
    "# tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should wait for (usually) several minutes. You can use those flags to understand if execution is complete and if it was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_execution.refresh_status().is_done()\n",
    "# tf_execution.refresh_status().is_successful()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classic (Pandas) transforation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All table names for your problem\n",
    "problem.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into pandas DataFrame\n",
    "df = problem.read_dataframe_from_table(\"please specify table name\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Change Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do changes using pandas\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Save data back to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check available parameters running the line below\n",
    "# problem.write_dataframe_into_table?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data\n",
    "# problem.write_dataframe_into_table(df, table_name=<new table name>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data to Required Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Last Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. If you use server-side transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select transformation class from Tfs.all() list, and replace <SELECT CLASS> with class name\n",
    "tf_final = Tfs.<SELECT CLASS>()\n",
    "# Last transformation in the chain should have the predefined name = \"target_time_series\"\n",
    "# That's required by our backend: we don't specify time series table name explicitly but rather expect data to be\n",
    "# present in a table named \"target_time_series\"\n",
    "tf_final.name = \"target_time_series\"\n",
    "tf_final.partitions = 1\n",
    "tf_final.problem_id = problem.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long running\n",
    "\n",
    "# Run transformation\n",
    "tf_execution = tf_final.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If notebook when offline while you've been waiting, you can always load the execution object from DB\n",
    "# tf_execution = TfExecution.instance(problem_id, transformation_id)\n",
    "\n",
    "# Wait until `is_done` is True\n",
    "tf_execution.refresh_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Or, If you use in-memory pandas transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df = problem.read_dataframe_from_table('t1_problem_120')\n",
    "df.head()\n",
    "# Do changes using pandas\n",
    "# ...\n",
    "# Write date\n",
    "# problem.write_dataframe_into_table(df, \"target_time_series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations BEFORE Train Test Split\n",
    "\n",
    "Usually Transformations which add features or modify existing features into new ones can be applied before Train Test Split.\n",
    "\n",
    "For example:\n",
    "- Absolute - Computes the absolute value of a number.\n",
    "- SelectColumns - Selects specified columns of interest from the table.\n",
    "\n",
    "Can be applied BEFORE the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations BEFORE splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and run this cell to apply train validation test split\n",
    "\n",
    "# train_val_test_splitter = Tfs.Split()\n",
    "# train_val_test_splitter.name = \"\"\n",
    "# train_val_test_splitter.ignoreHeader = True\n",
    "# train_val_test_splitter.table = \"\"\n",
    "# train_val_test_splitter.p = [0.65, 0.15, 0.20]\n",
    "# train_val_test_splitter.problem_id = problem.id()\n",
    "# train_val_test_splitter.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations AFTER Train Test Split\n",
    "\n",
    "Some Transformations should be applied after the Train Test Split separately as otherwise they can lead to Leakage of Data.\n",
    "\n",
    "These usually include Encoder, Scaler, Normalizer Transformations.\n",
    "\n",
    "For example:\n",
    "- OneHotEncode - Transforms the chosen columns into one-hot representations.\n",
    "- AnomalyDetection - Adds is_outlier column where 1 defines an anomaly\n",
    "\n",
    "Should be applied AFTER the Train Test Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run transformations AFTER splitting the data into train test\n",
    "# Use the examples as shown above to run the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "experiment = PredictConfiguration.load(problem_id=problem_id, experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Fields (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those fields are mandatory, and require you to set values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to [the XGBoost Documentation](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#learning-task-parameters) for all possible `learning_objective`s and `comparison_metric`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please give your experiment a meaninful name\n",
    "# It should be unique in the scope of the problem\n",
    "experiment.name = \"Demo\"\n",
    "experiment.description = \"demo\"\n",
    "\n",
    "#specify the dataset for the experiment\n",
    "# Format is : <train_test_split_transformation_name>_problem_<problem_id>_<i>\n",
    "# i=0 for train data, i=1 for valid data and i=2 for test data\n",
    "\n",
    "experiment.training_data_s3_uri = \"\"\n",
    "experiment.validation_data_s3_uri = \"\"\n",
    "experiment.test_data_s3_uri = \"\"\n",
    "\n",
    "# Valid Pairs include\n",
    "# binary:logistic -> error  | auc | aucpr\n",
    "# multi:softprob  -> merror | auc | aucpr\n",
    "# reg:logistic    -> rmse  | rmsle | mae | mape\n",
    "experiment.learning_objective = \"BinaryClassification\"\n",
    "experiment.comparison_metric = \"rmse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can leave the values as is, or adjust them if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment and change any of these default values\n",
    "\n",
    "# experiment.positive_data_weight: float = 1.0\n",
    "# experiment.hpt_max_jobs: int = 5\n",
    "# experiment.hpt_max_parallel_jobs: int = 5\n",
    "# experiment.train_instance_count: int = 1\n",
    "# experiment.train_instance_type: str = \"ml.m4.xlarge\"\n",
    "# experiment.test_instance_count: int = 1\n",
    "# experiment.test_instance_type: str = \"ml.m4.xlarge\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running training make sure\n",
    "  - first column contains the target value  [use : MoveColumnToFirstGlueTransformer]\n",
    "  - the dataset headers are extracted and uploaded to S3 [use : UpdateColumnsGlueTransformer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "experiment.save_and_run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Now all you need is to wait until training is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the status = 'Training complete', we can proceed further. Otherwise, we need to wait.\n",
    "# Training could take several hours, or even more, on huge datasets\n",
    "experiment = ExperimentLoader.load(problem_id, experiment_id)\n",
    "experiment.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find the model valuable, you might consider deploying it to reuse in the future.\n",
    "Uncomment and run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# Long-running job\n",
    "\n",
    "# experiment.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System cell\n",
    "# When deployment is done, you can run inferences using the inference link\n",
    "# experiment.inference_link()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda_amazonei_tensorflow_p36"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
